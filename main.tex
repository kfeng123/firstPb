\documentclass[review]{elsarticle}
 
\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\journal{Journal of Multivariate Analysis}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
%\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{color}
\usepackage{booktabs}
\usepackage[title]{appendix}
\usepackage[FIGTOPCAP]{subfigure}

\usepackage{ulem}

\DeclareMathOperator{\mytr}{tr}
\DeclareMathOperator{\mydiag}{diag}
\DeclareMathOperator{\myrank}{Rank}
\DeclareMathOperator{\myE}{E}
\DeclareMathOperator{\myVar}{Var}

\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\ud}{\mathbf{d}}



\newcommand{\Ba}{\mathbf{a}}    \newcommand{\Bb}{\mathbf{b}}    \newcommand{\Bc}{\mathbf{c}}    \newcommand{\Bd}{\mathbf{d}}    \newcommand{\Be}{\mathbf{e}}    \newcommand{\Bf}{\mathbf{f}}    \newcommand{\Bg}{\mathbf{g}}    \newcommand{\Bh}{\mathbf{h}}    \newcommand{\Bi}{\mathbf{i}}    \newcommand{\Bj}{\mathbf{j}}    \newcommand{\Bk}{\mathbf{k}}    \newcommand{\Bl}{\mathbf{l}}
\newcommand{\Bm}{\mathbf{m}}    \newcommand{\Bn}{\mathbf{n}}    \newcommand{\Bo}{\mathbf{o}}    \newcommand{\Bp}{\mathbf{p}}    \newcommand{\Bq}{\mathbf{q}}    \newcommand{\Br}{\mathbf{r}}    \newcommand{\Bs}{\mathbf{s}}    \newcommand{\Bt}{\mathbf{t}}    \newcommand{\Bu}{\mathbf{u}}    \newcommand{\Bv}{\mathbf{v}}    \newcommand{\Bw}{\mathbf{w}}    \newcommand{\Bx}{\mathbf{x}}
\newcommand{\By}{\mathbf{y}}    \newcommand{\Bz}{\mathbf{z}}    
\newcommand{\BA}{\mathbf{A}}    \newcommand{\BB}{\mathbf{B}}    \newcommand{\BC}{\mathbf{C}}    \newcommand{\BD}{\mathbf{D}}    \newcommand{\BE}{\mathbf{E}}    \newcommand{\BF}{\mathbf{F}}    \newcommand{\BG}{\mathbf{G}}    \newcommand{\BH}{\mathbf{H}}    \newcommand{\BI}{\mathbf{I}}    \newcommand{\BJ}{\mathbf{J}}    \newcommand{\BK}{\mathbf{K}}    \newcommand{\BL}{\mathbf{L}}
\newcommand{\BM}{\mathbf{M}}    \newcommand{\BN}{\mathbf{N}}    \newcommand{\BO}{\mathbf{O}}    \newcommand{\BP}{\mathbf{P}}    \newcommand{\BQ}{\mathbf{Q}}    \newcommand{\BR}{\mathbf{R}}    \newcommand{\BS}{\mathbf{S}}    \newcommand{\BT}{\mathbf{T}}    \newcommand{\BU}{\mathbf{U}}    \newcommand{\BV}{\mathbf{V}}    \newcommand{\BW}{\mathbf{W}}    \newcommand{\BX}{\mathbf{X}}
\newcommand{\BY}{\mathbf{Y}}    \newcommand{\BZ}{\mathbf{Z}}    

\newcommand{\bfsym}[1]{\ensuremath{\boldsymbol{#1}}}

\def\balpha{\bfsym \alpha}
\def\bbeta{\bfsym \beta}
\def\bgamma{\bfsym \gamma}             \def\bGamma{\bfsym \Gamma}
\def\bdelta{\bfsym {\delta}}           \def\bDelta {\bfsym {\Delta}}
\def\bfeta{\bfsym {\eta}}              \def\bfEta {\bfsym {\Eta}}
\def\bmu{\bfsym {\mu}}                 \def\bMu {\bfsym {\Mu}}
\def\bnu{\bfsym {\nu}}
\def\btheta{\bfsym {\theta}}           \def\bTheta {\bfsym {\Theta}}
\def\beps{\bfsym \varepsilon}          \def\bepsilon{\bfsym \varepsilon}
\def\bsigma{\bfsym \sigma}             \def\bSigma{\bfsym \Sigma}
\def\blambda {\bfsym {\lambda}}        \def\bLambda {\bfsym {\Lambda}}
\def\bomega {\bfsym {\omega}}          \def\bOmega {\bfsym {\Omega}}
\def\brho   {\bfsym {\rho}}
\def\btau{\bfsym {\tau}}
\def\bxi{\bfsym {\xi}}
\def\bzeta{\bfsym {\zeta}}
% May add more in future.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







\theoremstyle{plain}
\newtheorem{theorem}{\quad\quad Theorem}
\newtheorem{proposition}{\quad\quad Proposition}
\newtheorem{corollary}{\quad\quad Corollary}
\newtheorem{lemma}{\quad\quad Lemma}
\newtheorem{example}{Example}
\newtheorem{assumption}{\quad\quad Assumption}
\newtheorem{condition}{\quad\quad Condition}

\theoremstyle{definition}
\newtheorem{remark}{\quad\quad Remark}
\theoremstyle{remark}


\begin{document}

\begin{frontmatter}

\title{High-dimensional two-sample test under spiked covariance}

%% Group authors per affiliation:
    \author[mymainaddress]{Rui Wang}
    \author[mymainaddress,mysecondaryaddress]{Xingzhong Xu\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead{xuxz@bit.edu.cn}
    \address[mymainaddress]{School of Mathematics and Statistics, Beijing Institute of Technology, Beijing 
    100081,China}
    \address[mysecondaryaddress]{Beijing Key Laboratory on MCAACI, Beijing Institute of Technology, Beijing 100081,China}
%\fntext[myfootnote]{Since 1880.}

%% or include affiliations in footnotes:
%\author[mymainaddress,mysecondaryaddress]{Elsevier Inc}
%\ead[url]{www.elsevier.com}



\begin{abstract}
    This paper considers testing the means of two $p$-variate normal samples in high dimensional setting.
    The covariance matrix is assumed to be spiked, which often arises in practice. 
    We derive the asymptotic distribution of~\cite{Chen2010A}'s test statistic under spiked covariance.
    Also, a new test procedure is proposed through projection on the orthogonal complement of principal space.
    The asymptotic normality of the new test statistic is proved and the power function of the test is given.
    Theoretical and simulation results show that the new test outperforms existing methods substantially when the covariance matrix is spiked.
\end{abstract}

\begin{keyword}
    high dimension, mean test, orthogonal complement of principal space, spiked covariance
\end{keyword}

\end{frontmatter}

%\linenumbers



\section{Introduction}

Suppose $X_{k,1},\ldots,X_{k,n_k}$  are independent identically distributed (i.i.d.) $p$-dimensional normal random vectors with unknown mean vector $\mu_k$ and covariance matrix $\bSigma$, $k=1,2$. We consider the hypothesis testing problem
\begin{equation}\label{problem}
    H_0:\mu_1=\mu_2\quad \textrm{vs.}\quad H_1:\mu_1\neq \mu_2.
\end{equation}
 In this paper, {the} high dimensional setting is adopted, that is, the dimension $p$ varies as $n$ increases, where $n=n_1+n_2$ is the total sample size.
Testing hypotheses~\eqref{problem} is important in many fields, including biology, finance and economics.


A classical test statistic for hypotheses~\eqref{problem} is Hotelling's $T^2$ test  statistic ${(\bar{X}_1-\bar{X}_2)}^T \BS^{-1}(\bar{X}_1-\bar{X}_2)$, where $\bar{X}_k=n_k^{-1}\sum_{i=1}^{n_k}X_{k,i}$ is the mean vector of sample $k$, $k=1,2$, and
    $
    \BS=
{(n-2)}^{-1}\sum_{k=1}^2\sum_{i=1}^{n_k} (X_{k,i}-\bar{X}_k) {(X_{k,i}-\bar{X}_k)}^T
    %{(n-2)}^{-1}((n_1-1)S_1+(n_2-1)S_2)
    $
is the pooled sample covariance matrix.
However, Hotelling's test statistic is not defined when $p\geq n-1$.
Moreover,~\cite{Bai1996Efiect} showed that even if $p<n-1$, Hotelling's test suffers from low power when $p$ is comparable to $n$.
Perhaps, the main reason for the low power of Hotelling's test is that $S$ is a poor estimator of $\bSigma$ is large compared with $n$.
See~\cite{Chen2010A} and the references therein.
For testing hypotheses~\eqref{problem} in high dimensional settings,  
many test statistics are based on the estimation of  ${(\mu_1-\mu_2)}^T \BA(\mu_1-\mu_2)$ for a positive definite matrix $\BA$.~\cite{Bai1996Efiect} proposed a test based on
\begin{equation*}
    T_{BS}=\|\bar{X}_1-\bar{X}_2\|^2-(\frac{1}{n_1}+\frac{1}{n_2})\mytr \BS,
\end{equation*}
an unbiased estimator of $\|\mu_1-\mu_2\|^2$.
\cite{Chen2010A} modified $T_{BS}$ by removing terms $\sum_{i=1}^{n_k}X_{ki}^T X_{ki}$, $k=1,2$, and proposed a test based on
\begin{equation*}
    \begin{aligned}
        T_{CQ}&=\frac{\sum_{i\neq j}^{n_1}X_{1i}^T X_{1j}}{n_1(n_1-1)}+\frac{\sum_{i\neq j}^{n_2}X_{2i}^T X_{2j}}{n_2(n_2-1)}-2\frac{\sum_{i=1}^{n_1}\sum_{j=1}^{n_2}X_{1i}^T X_{2j}}{n_1n_2}
        \\
            &=\|\bar{X}_1-\bar{X}_2\|^2-\frac{1}{n_1}\mytr \BS_1-\frac{1}{n_2}\mytr \BS_2,
    \end{aligned}
\end{equation*}
where
$\BS_k={(n_k -1)}^{-1}\sum_{i=1}^{n_k} (X_{k,i}-\bar{X}_k) {(X_{k,i}-\bar{X}_k)}^T
$, $k=1,2$.
As an estimator of $\|\mu_1-\mu_2\|^2$, $T_{CQ}$ is unbiased even if the covariances of two samples are different,
while $T_{BS}$ is unbiased only when the covariances are the same or $n_1=n_2$.
\cite{Srivastava2008A} proposed a test based on
\begin{equation*}
    T_{SD}={(\bar{X}_1-\bar{X}_2)}^T {[\mathrm{diag}(\BS)]}^{-1}(\bar{X}_1-\bar{X}_2),
\end{equation*}
where $\textrm{diag} (\BS)$ is a diagonal matrix with the same diagonal elements as $\BS$'s.
%To characterize strong correlation between variables,~\cite{Ma2015A} adopted a factor model proposed a test based on
%\begin{equation}\label{compete2}
 %    T_{FAST}=\frac{n_1 n_2}{n_1+n_2}\|\bar{X}_1-\bar{X}_2\|^2-(\mathrm{tr} S- \sum_{i=1}^{\hat{r}} \lambda_l(S))
%\end{equation}

As~\cite{Ma2015A} pointed out, however, the asymptotic properties of these test procedures may not be valid if strong correlations exist. For example, the condition 
%$$
%\mathrm{tr}(\bSigma_i \bSigma_j \bSigma_l \bSigma_h)=o[\mathrm{tr}^2\{{(\bSigma_1+\bSigma_2)}^2\}]\quad\quad  \textrm{for}\, i,j,l,h=1\,\textrm{or}\,2
%$$ 
\begin{equation}\label{chenscondition}
    \mathrm{tr}(\bSigma^4)=o\big(\mathrm{tr}^2(\bSigma^2)\big)
\end{equation}
adopted by~\cite{Chen2010A} is violated when $\bSigma$ has a uniform correlation structure, that is, $\bSigma=(1-\rho)\BI_p+\rho\textbf{1}_p \textbf{1}_p^T$ where $0<\rho<1$, $\BI_p$ is the $p$ dimensional identity matrix and $\bm{1}_p$ is the $p$ dimensional vector  with elements $1$.
In this case, $\bSigma$ has eigenvalues $1+\rho(p-1) $ and $1-\rho$ with multiplicities $1$ and $p-1$ respectively. Then~\eqref{chenscondition} is violated since 
$$
\frac{\mytr (\bSigma^4)}{\mytr^2(\bSigma^2)}=\frac{\big(1+\rho(p-1)\big)^4 + (1-\rho)^4 (p-1)}{\big[ \big(1+\rho(p-1)\big)^2+(1-\rho)^2 (p-1) \big]^2}\to 1
$$
as $p\to \infty$.
  Under the uniform correlation structure, the leading eigenvalue of $\bSigma$ is significantly larger than the rest of eigenvalues.
  This is a special case of the spiked covariance model
 \begin{equation}\label{eq:introSpiked}
 \bSigma =\BV\bLambda \BV^T +\sigma^2 \BI_p,
 \end{equation}
 where $\bLambda=\mydiag(\blambda_1,\ldots,\blambda_r)$, $\blambda_1\geq \cdots\geq \blambda_r>0$, $r\geq 1$, $\BV$ is a $p\times r$ orthonormal matrix and $\sigma^2>0$.
 %Under spiked covariance model, the first $r$ eigenvalues of $\bSigma$ are distinguishable larger than the rest eigenvalues.
The spiked covariance model~\eqref{eq:introSpiked} is adopted by many theoretical studies, see~\cite{Cai2012Sparse},~\cite{Birnbaum2013},~\cite{Passemier2015} and the references therein.
The spiked covariance arises when variables are strongly correlated and the correlations are determined by a small number of factors.



Strong correlations between variables do exist in practice. In gene expression analysis, genes are correlated due to genetic regulatory networks (see~\cite{Thulin2014A}).~\cite{Chen2011A} pointed out that in terms of pathway analysis in proteomic studies,  test level can not be guaranteed if correlations are incorrectly assumed to be absent.
 As~\cite{Ma2015A} argued, there're strong correlations between different stock returns since they are all affected by the market index.
In section~\ref{sec:chen}, it will be seen that the asymptotic normality of $T_{CQ}$ is not valid when $\lambda_i$'s in~\eqref{eq:introSpiked} are large.
%We find that incorrectly assuming the absence of correlation between variables will result in level inflation.
%Hence we modify the critical value such that the resulting test procedure has correct asymptotic level under spiked covariance.
 %We derived the asymptotic distribution of $T_{CQ}$ under spiked covariance model. 
Generally, the asymptotic distribution of $T_{CQ}$ is the distribution of a weighted sum of chi-squared random variables.
In a special case, the asymptotic distribution is the distribution of a weighted sum of chi-squared random variables and a normal random variable. 

Recently, a class of test procedures are proposed through random projection. See~\cite{Lopes2015A},~\cite{Thulin2014A} and~\cite{Srivastava2014RAPTT}. The idea is to project data on some random lower-dimensional subspaces. It has been shown that these
procedures perform well under strong correlations. 
The random projection methods imply that test procedures are improved when data are projected on certain subspaces.
We will see that the ideal subspace is the orthogonal complement of the principal space.
Fortunately, the principal space can be estimated consistently even in high dimensional setting by the theory of principal component analysis (PCA).
%We find the ideal subspace is the orthogonal complement of the principal space.
%In this case, we know from the theory of principal component analysis (PCA) that the principal space can be estimated consistently even in high dimensional setting.
With the assumption of spiked covariance model, we propose a new test procedure through projection on the (estimated) ideal subspace.  
The asymptotic null distribution of the test statistic is derived and asymptotic power is also given.
%We will see that the asymptotic power function increases fast. In fact, the increasing rate is of a higher order than that of $T_{CQ}$.
We will see that the test is more powerful than $T_{CQ}$.
%Simulation study justifies the well-performance of the new test. Our theoretical results need the assumption $\sqrt{p}/(n_1+n_2)\to 0$. Simulation study shows that if it doesn't converge to $0$, the theorem may not be valid.

%{\color{red}{To the best of our knowledge,~\cite{Ma2015A} and~\cite{2016arXiv160202491A} are the only work concerned on problem (~\eqref{problem}) when strong correlation exists.
%\cite{Ma2015A} adopted a factor model and modified the test statistic of~\cite{Chen2010A} to guarantee the test level. But we will see that the test still suffers from low power. In an independent working paper,~\cite{2016arXiv160202491A} adopted a spiked covariance structure, and their statistic is similar to ours. The main advantage of our work is that our theorems don't need strict relationship between $p$ and $n$. And our statistic is invariant under shift.
%}}


%{\color{red}{A fairly recent work~\cite{2016arXiv160202491A} proposed a new test for strongly spiked eigenvalue model. The proposed a test based on an estimation of
%\begin{equation}
%    \begin{aligned}
%        T_{AY}=&\frac{\sum_{i\neq j}^{n_1}X_{1i}^T\tilde{\BV}_1\tilde{\BV}_1^T X_{1j}}{n_1(n_1-1)}+\frac{\sum_{i\neq j}^{n_2}X_{2i}^T\tilde{\BV}_1\tilde{\BV}_1^T X_{2j}}{n_2(n_2-1)}
%        \\&-2\frac{\sum_{i=1}^{n_1}\sum_{j=1}^{n_2}X_{1i}^T\tilde{\BV}_1\tilde{\BV}_1^T\tilde{\BV}_2\tilde{\BV}_2^T X_{2j}}{n_1n_2}
%    \end{aligned}
%\end{equation}
%which is similar to our statistic in form. However, the theory framework is different. And we will see our statistic is different from theirs in some key properties.
%}}




The rest of the paper is organized as follows. In Section 2,  we revisit~\cite{Chen2010A}'s test.  In Section 3, we propose a test procedure and exploit properties of the test.  In Section 4, simulations are carried out and  a real data example is given. Section 5 contains some discussion. All the technical details are in appendix.

\section{Asymptotic properties of~\cite{Chen2010A}'s test}\label{sec:chen}
%\textcolor{red}{Consider testing hypotheses~\eqref{problem} under covariance matrix~\eqref{eq:introSpiked}.}
    %Let $X_{k,1},\ldots, X_{k, n_k}$  be i.i.d.\ observations with common distribution $N(\mu_k,\bSigma)$, $k=1,2$.
    Throughout the paper, we  assume $p\to \infty$ as $n\to \infty$ and ${n_1}/{n_2}\to c \in (0,+\infty)$, that is, we consider high dimensional and balanced data.



    In~\cite{Chen2010A}, the asymptotic normality of $T_{CQ}$ is derived under the condition~\eqref{chenscondition}.
    We shall show that under the null hypothesis, the condition~\eqref{chenscondition} is essential for the asymptotic normality of $T_{CQ}$.
We note that under the null hypothesis, $T_{CQ}$ is a quadratic form of a standard normal random vector.
To see this,
let $Z_{k,i}=\bSigma^{-1/2}X_{k,i}$, $k=1,2$, $i=1,\ldots,n_k$.
It can be seen that $Z_{k,i}$ is $ N_p(0,\BI_{p})$ distributed under the null hypothesis.
Write $Z=(Z_{1,1}^T,\ldots,Z_{1,n_1}^T,Z_{2,1}^T,\ldots,Z_{2,n_2}^T)^T$.
    Then 
        $
        T_{CQ}=Z^T \big( \BB_n\otimes \bSigma \big) Z
        $,
    where $\otimes$ is the Kronecker product and
    \begin{equation*}
        \BB_n=\begin{pmatrix}
            \frac{1}{n_1(n_1-1)}(\mathbf{1}_{n_1} \mathbf{1}_{n_1}^T-\BI_{n_1})&
            -\frac{1}{n_1 n_2}\mathbf{1}_{n_1} \mathbf{1}_{n_2}^T\\
            -\frac{1}{n_1 n_2}\mathbf{1}_{n_2} \mathbf{1}_{n_1}^T&
            \frac{1}{n_2(n_2-1)}( \mathbf{1}_{n_2} \mathbf{1}_{n_2}^T-\BI_{n_2})\\
        \end{pmatrix}.
    \end{equation*}
    
Using characteristic function method, one can prove the following result which gives a necessary and sufficient condition for the asymptotic normality of the quadratic form of a standard normal random vector.
\begin{lemma}\label{quadraticFormCLT}
    Suppose $Y_{n}$ is a $k_n$ dimensional standard normal random vector and $\BA_n$ is a $k_n\times k_n$ symmetric matrix. Then as $n\to \infty$, a necessary and sufficient condition for
    \begin{equation}\label{quadratic}
        \frac{Y_n^T \BA_n Y_n-\myE Y_n^T \BA_n Y_n}{{[\myVar (Y_n^T \BA_n Y_n)]}^{1/2}}\xrightarrow{\mathcal{L}}N(0,1)
    \end{equation}
    is that
    \begin{equation}\label{quadraticEigen}
        \frac{\lambda_{1}(\BA_n^2)}{\mytr(\BA_n^2)}\to 0,
    \end{equation}
    where ``$\xrightarrow{\mathcal{L}}$" means convergence of a sequence of random variables in law and $\lambda_{i}(\cdot)$ means the $i$th largest eigenvalue.
\end{lemma}

To apply Lemma~\ref{quadraticFormCLT} to $T_{CQ}$, one needs to calculate the eigenvalues of $\BB_n\otimes \bSigma$.
    Note that the eigenvalues of $\BB_n$ are $-1/n_1(n_1-1)$, $-1/n_2(n_2-1)$, $(n_1+n_2)/n_1 n_2$ and $0$ with multiplicities $n_1-1$, $n_2-1$, $1$ and $1$ respectively.
    %The eigenspace corresponding to $-1/n_1(n_1-1)$ is
    %\begin{equation*}
        %\{{(\eta^T,\underbrace{0,\ldots,0}_{n_2})}^T|\, \textrm{$\eta\in \mathbb{R}^{n_1}$ and $\eta^T \gamma_1=0$}\}.
    %\end{equation*}
    %The eigenspace corresponding to $-1/n_2(n_2-1)$ is
    %\begin{equation*}
        %\{{(\underbrace{0,\ldots,0}_{n_1},\eta^T)}^T| \, \textrm{$\eta\in\mathbb{R}^{n_2}$ and $\eta^T \gamma_2=0$}\}.
    %\end{equation*}
    %The eigenvectors corresponding to $(n_1+n_2)/n_1n_2$ and $0$ are respectively
    %\begin{equation*}
        %\Big(-\sqrt{\frac{n_2}{n_1+n_2}}\gamma_1^T, \sqrt{\frac{n_1}{n_1+n_2}}\gamma_2^T\Big)^T
        %\quad\text{and}\quad
        %\Big(\sqrt{\frac{n_1}{n_1+n_2}}\gamma_1^T, \sqrt{\frac{n_2}{n_1+n_2}}\gamma_2^T\Big)^T.
    %\end{equation*}
    Thus,
    \begin{equation*}
        \mytr{(\BB_n\otimes \bSigma)}^2=\mytr(\BB_n^2)\mytr\bSigma^2=(\frac{1}{n_1(n_1-1)}+\frac{1}{n_2(n_2-1)}+\frac{2}{n_1 n_2})\mytr\bSigma^2,
    \end{equation*}
and
    \begin{equation*}
        \lambda_{1}\Big({(\BB_n\otimes \bSigma)}^2\Big)=\lambda_{1}(\BB_n^2)\lambda_{1}(\bSigma^2)={\Big(\frac{1}{n_1}+\frac{1}{n_2}\Big)}^2\lambda_{1}(\bSigma^2).
    \end{equation*}
    Because $n_1/n_2\to c$, the condition
    $$
         \frac{\lambda_{1}\Big({(\BB_n\otimes \bSigma)}^2\Big)}{
\mytr{\big(\BB_n\otimes \bSigma\big)}^2}
         \to 0
    $$
     is equivalent to $\lambda_{1}(\bSigma^2)/\mytr\bSigma^2\to 0$.
From
$$
\frac{\lambda_1(\bSigma)^4}{(\sum_{i=1}^p \lambda_i(\bSigma)^2)^2}
\leq
\frac{\sum_{i=1}^p\lambda_i(\bSigma)^4}{(\sum_{i=1}^p \lambda_i(\bSigma)^2)^2}
\leq
\frac{\lambda_1(\bSigma)^2\sum_{i=1}^p\lambda_i(\bSigma)^2}{(\sum_{i=1}^p \lambda_i(\bSigma)^2)^2}
=
\frac{\lambda_1(\bSigma)^2}{\sum_{i=1}^p \lambda_i(\bSigma)^2},
$$
    we can see that ${\lambda_{1}^2(\bSigma)}/{\mathrm{tr}(\bSigma^2)}\to 0$  is equivalent to~\eqref{chenscondition}.
Then Lemma~\ref{quadraticFormCLT} implies that under the null hypothesis, the condition~\eqref{chenscondition} is a necessary and sufficient condition for 
    \begin{equation*}
        \frac{T_{CQ}-\myE T_{CQ}}{{\big[\mathrm{Var}(T_{CQ})\big]}^{1/2}}\xrightarrow{\mathcal{L}}N(0,1).
    \end{equation*}

    The above result implies that~\cite{Chen2010A}'s test procedure can be used only when the eigenvalues of $\bSigma$ are concentrated around their average. 
   In a class of applications, however, the correlations between variables are mainly driven by several common factors, and consequently, $\bSigma$ has a few eigenvalues which are much larger than the others.
    See, for example,~\cite{Jung2009PCA},~\cite{Cai2012Sparse} and~\cite{Fan2015Asymptotics}.
To characterize such correlations between variables, we consider the  spiked covariance structure~\eqref{eq:introSpiked}.
For $p\geq q$, let $\mathbb{O}_{p\times q}$ denote the collection of all $p\times q$ column orthogonal matrices.
We make the following assumption for the covariance matrix $\bSigma$.
\begin{assumption}\label{theModel}
    The covariance matrix $\bSigma$ has structure $ \bSigma=\BV\bLambda \BV^T+\sigma^2 \BI_p$, where $\BV\in\mathbb{O}_{p\times r}$, $r$ is a known number and $\bLambda=\mydiag(\blambda_{1},\ldots,\blambda_{r})$, 
 $\blambda_{1}\geq \cdots \geq \blambda_{r}>0$.
As $n$, $p$ tend to infinity, the parameters
$r$, $\sigma^2$ are fixed and $\bLambda$ satisfies     
    \begin{equation*}
        \kappa p^{\beta}\geq \blambda_{1}\geq \cdots \geq\blambda_{r}\geq \kappa^{-1}p^{\beta},
\end{equation*}
where $\kappa>1$ and $\beta\geq {1}/{2}$ are constants.
\end{assumption}

The covariance structure in Assumption~\ref{theModel} is commonly adopted in PCA study.
See~\cite{Cai2012Sparse},~\cite{Birnbaum2013},~\cite{Passemier2015} and the references therein.
This covariance structure is also connected with the factor model.
In fact, the model in Assumption~\ref{theModel} with $\beta=1$
corresponds to the factor model in~\cite{Ma2015A} with homoscedastic noise.

In Assumption~\ref{theModel}, the column space of $\BV$ is the eigenspace of $\bSigma$ associated with the $r$ leading eigenvalues, and is therefore called principal space. Since $\BV$ is a column orthogonal matrix, $\BV \BV^T$ is the orthogonal projection onto the principal space.
Let $\tilde{\BV}$ be a member of $\mathbb{O}_{p\times (p-r)}$ such that the columns of $\tilde{\BV}$ are orthogonal to the columns of  $\BV$.
%, that is $\tilde{\BV}_k^T \BV_k=\BO_{r_k\times(p-r_k )}$
 Although such $\tilde{\BV}$ is not unique, the orthogonal projection  $\tilde{\BV}\tilde{\BV}^T=\BI_p-\BV \BV^T$ is unique and is equal to the orthogonal projection onto the orthogonal complement of principal space.


For positive sequences $\{a_n\}$ and $\{b_n\}$, we write $a_n\asymp b_n$ to denote $a_n=O(b_n)$ and $b_n=O(a_n)$ as $n\to \infty$.
Under Assumption~\ref{theModel}, we have
$$
\frac{\mytr(\bSigma^4)}{\mytr^2(\bSigma^2)}=
\frac{\sum_{i=1}^r (\blambda_i+\sigma^2)^4+(p-r)\sigma^8}{\big(\sum_{i=1}^r (\blambda_i+\sigma^2)^2+(p-r)\sigma^4\big)^2}\asymp \frac{p^{4\beta}+p}{(p^{2\beta}+p)^2}.
$$
The right hand side tends to $0$ if and only if $\beta<1/2$.
Our previous arguments assert that the asymptotic distribution of $T_{CQ}$ won't be normal for $\beta\geq 1/2$.
To derive the asymptotic distribution of $T_{CQ}$ for $\beta\geq 1/2$,
note that the variation of $T_{CQ}$ is mainly due to $\|\bar{X}_1-\bar{X}_2\|^2$.
Let $\tau=1/n_1+1/n_2$.
Under the null hypothesis, we have
$$
\myVar(\|\bar{X}_1-\bar{X}_2\|^2)=
2\tau^2 \mytr (\bSigma^2)=2\tau^2 \sum_{i=1}^r (\blambda_i+
\sigma^2)^2
+ 2\tau^2 (p-r)\sigma^4,
$$
where the first term of the right hand side is of order $p^{2\beta}/n^2$ and the second term is of order $p/n^2$.
If $\beta=1/2$, the two terms are of the same order. 
If $\beta>1/2$, however, the second term is dominated by the first term.
This implies that the asymptotic distributions of $T_{CQ}$ are different for $\beta=1/2$ and $\beta>1/2$.
Since the variance of $(\tau p^{\beta})^{-1}\|\bar{X}_1-\bar{X}_2\|^2$ is bounded under the null hypothesis, we use $\tau p^{\beta}$ to standardize $T_{CQ}$.
The following two theorems give the asymptotic distributions of $(\tau p^{\beta})^{-1}T_{CQ}$ when $\beta= 1/2$ and $\beta>1/2$, respectively.
\begin{theorem}\label{Chenstheory1}
 Under Assumption~\ref{theModel},
    suppose $\beta=1/2$ and $\lambda_i/p^\beta \to \omega_i\in(0,+\infty)$, $i=1,\ldots,r$.
    Let $Z_{0},Z_1,\ldots,Z_{r}$ be i.i.d.\ $N(0,1)$ random variables,
     then the following results hold:
     \begin{enumerate}[(a)]
         \item
             If $\mu_1=\mu_2$, then
    $$
        \frac{1}{\tau p^{\beta}} T_{CQ}
        \xrightarrow{w}
\sqrt{2}\sigma^2 Z_0
+
        \sum_{i=1}^r \omega_i Z_i^2
            -
        \sum_{i=1}^r \omega_i,
    $$
             where ``$\xrightarrow{w}$'' denotes weak convergence.
         \item
             If $(\tau p^{\beta})^{-1/2}\big(\BV^T (\mu_1-\mu_2)\big)_i\to \zeta_i\in(-\infty,+\infty)$, $i=1,\ldots,r$,
             and
    ${(\tau p^\beta)}^{-1}\|\tilde{\BV}^T (\mu_1-\mu_2)\|^2\to \zeta^*\in [0,+\infty)$, then
    $$
        \frac{1}{\tau p^{\beta}} T_{CQ}
        \xrightarrow{w}
\sqrt{2}\sigma^2 Z_0+
        \sum_{i=1}^r (\sqrt{\omega_i} Z_i+\zeta_i)^2+
\zeta^*
        -
        \sum_{i=1}^r \omega_i.
    $$
     \end{enumerate}
\end{theorem}

\begin{theorem}\label{Chenstheory2}
 Under Assumption~\ref{theModel},
    suppose $\beta>1/2$ and $\lambda_i/p^\beta \to \omega_i\in(0,+\infty)$, $i=1,\ldots,r$.
    Let $Z_1,\ldots,Z_{r}$ be i.i.d.\ $N(0,1)$ random variables,
     then the following results hold:
     \begin{enumerate}[(a)]
         \item
             If $\mu_1=\mu_2$, then
    $$
        \frac{1}{\tau p^{\beta}} T_{CQ}
        \xrightarrow{w}
        \sum_{i=1}^r \omega_i Z_i^2
            -
        \sum_{i=1}^r \omega_i.
    $$
         \item
             If $(\tau p^{\beta})^{-1/2}\big(\BV^T (\mu_1-\mu_2)\big)_i\to \zeta_i\in(-\infty,+\infty)$, $i=1,\ldots,r$,
             and
    ${(\tau p^\beta)}^{-1}\|\tilde{\BV}^T (\mu_1-\mu_2)\|^2\to \zeta^*\in [0,+\infty)$, then
    $$
        \frac{1}{\tau p^{\beta}} T_{CQ}
        \xrightarrow{w}
        \sum_{i=1}^r (\sqrt{\omega_i} Z_i+\zeta_i)^2+
\zeta^*
        -
        \sum_{i=1}^r \omega_i.
    $$
     \end{enumerate}
\end{theorem}

%\begin{theorem}\label{Chenstheory}
 %Under Assumption~\ref{theModel},
    %suppose $\lambda_i/p^\beta \to \omega_i\in(0,+\infty)$, $i=1,\ldots,r$.
    %Let $\tau=1/n_1+1/n_2$. We further assume
    %$(\tau p^{\beta})^{-1/2}\big(\BV^T (\mu_1-\mu_2)\big)_i\to \zeta_i\in(-\infty,+\infty)$, $i=1,\ldots,r$, and
    %${(\tau p^\beta)}^{-1}\|\tilde{\BV}^T (\mu_1-\mu_2)\|^2\to \zeta^*\in [0,+\infty)$. We have
    %$$
        %\frac{1}{\tau p^{\beta}} T_{CQ}
        %\xrightarrow{\mathcal{L}}
        %\sum_{i=1}^r (\sqrt{\omega_i} Z_i+\zeta_i)^2+
%\zeta^*+
    %\sqrt{2}\sigma^2 \delta_{\{\frac{1}{2}\}}(\beta)\epsilon
        %-
        %\sum_{i=1}^r \omega_i,
    %$$
    %where $Z_{1},\ldots,Z_{r}$ and $\epsilon$ are i.i.d.\ $N(0,1)$ random variables, $\delta_{\{\frac{1}{2}\}}(\beta)$ equals to $1$ if $\beta=1/2$ and $0$ otherwise.
%\end{theorem}
\begin{remark}\label{remark1}
    By the definitions of $\zeta_i$ and $\zeta^*$, we have
    $$
    \begin{aligned}
        &\frac{1}{\tau p^{\beta}}\|\mu_1-\mu_2\|^2
    =
    \frac{1}{\tau p^{\beta}}\|\BV^T (\mu_1-\mu_2)\|^2
        +
        \frac{1}{\tau p^{\beta}}\|\tilde{\BV}^T (\mu_1-\mu_2)\|^2
    \to \sum_{i=1}^r \zeta_i^2+ \zeta^*.
    \end{aligned}
    $$
    Thus, $\sum_{i=1}^r\zeta_i^2$ and $\zeta^*$ characterize the signal strength in the principal space and the complement of the principal space, respectively.
    Under the conditions of Theorem~\ref{Chenstheory1} or Theorem~\ref{Chenstheory2}, the following statements are equivalent:
    \begin{enumerate}[(1)]
        \item
     $\zeta_1=\cdots=\zeta_r=\zeta^*=0$.
 \item
     $\|\mu_1-\mu_2\|^2=o(\tau p^{\beta})$.
         \item
            The asymptotic distributions of $(\tau p^{\beta})^{-1}T_{CQ}$ are the same under the null hypothesis and the alternative hypothesis.
        \item
            Any test procedure based on $T_{CQ}$ has trivial power asymptotically.
    \end{enumerate}
\end{remark}


It is implied by Theorem~\ref{Chenstheory1} and Theorem~\ref{Chenstheory2} that the original critical value of $T_{CQ}$ can not be used when $\beta\geq 1/2$.
Now we adjust the critical value of $T_{CQ}$ such that the resulting test has correct level asymptotically.
Consider the random variable
$
W=
\sqrt{2p}\sigma^2 Z_0
+
        \sum_{i=1}^r \blambda_i Z_i^2
            -
        \sum_{i=1}^r \blambda_i
        $, 
where $Z_0,Z_1,\ldots,Z_r$ are i.i.d.\ $N(0,1)$ random variables.
Let $F(x;\blambda_1,\ldots,\blambda_r,\sigma^2)$ be the cumulative distribution function of $W$.
Under the conditions of Theorem~\ref{Chenstheory1}, we have
$$
\frac{W}{p^{\beta}}\xrightarrow{w}
\sqrt{2}\sigma^2 Z_0 + \sum_{i=1}^r \omega_i Z_i^2 -\sum_{i=1}^r \omega_i.
$$
Under the conditions of Theorem~\ref{Chenstheory2}, we have
$$
\frac{W}{p^{\beta}}\xrightarrow{w}
\sum_{i=1}^r \omega_i Z_i^2 -\sum_{i=1}^r \omega_i.
$$
Hence in both case, we have
$$
\sup_{x\in\mathbb{R}}\big|\Pr\big(\frac{1}{\tau}T_{CQ}\leq x\big)-\Pr\big(W\leq x\big)\big|
=o(1).
$$
Thus, if we reject the null hypothesis when
$$
\frac{1}{\tau}T_{CQ}>
F^{-1}(1-\alpha;\blambda_1,\ldots,\blambda_r,\sigma^2),
$$
then the resulting test is asymptotically level $\alpha$ for $\beta\geq 1/2$.
        However, the distribution $F(x;\blambda_1,\ldots,\blambda_r,\sigma^2)$ involves some unknown parameters.
In order to consistently estimate $F(x;\blambda_1,\ldots,\blambda_r,\sigma^2)$, we need to estimate $\blambda_1,\ldots,\blambda_r$ and $\sigma^2$.
In section~\ref{methodology}, we will give their estimators $\hat{\blambda}_1,\ldots,\hat{\blambda}_r$ and $\hat{\sigma}_{*}^2$. 
Proposition~\ref{varianceEstimation} asserts that these estimators are ratio consistent. 
Now we propose a corrected $T_{CQ}$ test procedure which reject the null hypothesis with $\alpha$ level of significance if
$$
\tau^{-1}{T_{CQ}}> F^{-1}(1-\alpha;\hat{\blambda}_1,\ldots,\hat{\blambda}_r,\hat{\sigma}_*^2).
$$
Then under the conditions of either Theorem~\ref{Chenstheory1} or Theorem~\ref{Chenstheory2}, the corrected $T_{CQ}$ test procedure is  asymptotically level $\alpha$.

As we have seen in Remark~\ref{remark1}, for the corrected $T_{CQ}$ test, the separation boundary between the testable and non-testable regions  is $\|\mu_1-\mu_2\|^2\asymp p^{\beta}/n$.
Then as $\beta$ increases, the corrected $T_{CQ}$ test procedure becomes less powerful.
This implies that the power of the corrected $T_{CQ}$ test procedure is negatively affected by the large eigenvalues of $\bSigma$.

%Theorems~\ref{Chenstheory1} and~\ref{Chenstheory2} imply that $T_{CQ}$ has trivial asymptotic power if $\zeta_1=\cdots=\zeta_r=\zeta^*=0$, which is equivalent to $\sum_{i=1}^r \zeta_i^2 +\zeta^*=0$, or equivalently,
%$$
%\frac{1}{\tau p^\beta}\|\mu_1-\mu_2\|^2\to 0.
%$$
%Conversely, to make $T_{CQ}$ have non-trivial power, $\|\mu_1-\mu_2\|^2$ is at least of order $\tau p^\beta$.




\section{A projection test}\label{methodology}


In Section~\ref{sec:chen}, we adjusted the critical value of $T_{CQ}$ such that the corrected $T_{CQ}$ test procedure is asymptotically level $\alpha$ under Assumption~\ref{theModel}.
However, % as we discussed at the end of Section~\ref{sec:chen},
the power of the corrected $T_{CQ}$ test procedure is negatively affected by the large eigenvalues of $\bSigma$.
%To this end,~\cite{Ma2015A} proposed a test procedure which is based on $T_{CQ}$ and has the correct asymptotic test level.
%In their paper, $\bSigma$'s first few eigenvalues are assumed to be of order $p$.
This motivates us to propose a new test for the hypotheses~\eqref{problem} under Assumption~\ref{theModel}.

%The argument is also supported by the likelihood ratio test. If $\bSigma$ is known, the LRT is based on 
%\begin{equation}\label{qifafa}
    %{(\bar{X}_1-\bar{X}_2)}^T\bSigma^{-1}(\bar{X}_1-\bar{X}_2)=\frac{n_1 n_2}{n_1+n_2}\sum_{i=1}^p \lambda_i^{-1}{(\bar{X}_1-\bar{X}_2)}^T  p_i p_i^T (\bar{X}_1-\bar{X}_2).
%\end{equation}
%The difference between~\eqref{qifa} and~\eqref{qifafa} is the weights $\lambda_i^{-1}$.
%% For LRT, large $\lambda_i$'s corresponds to small weights in the sum.
%%If $\lambda_i$ is large, then the corresponding term has a small weight $\lambda_i^{-1}$ in the sum. 
%Unfortunately, $\lambda_i$'s are hard to precisely estimate in high dimensional setting. See~\cite{bai2010spectral} for detail. Nevertheless, it's possible to identify which $\lambda_i$'s are large. LRT implies the corresponding terms should have small weights, which coincides with our previous idea.
%If we assume there are correlations between $p$ variables, e.g. $\bSigma=(1-c)I+c\bm{1}_p \bm{1}_p^T$ where c is a constant fulfill $-\frac{1}{p-1}<c<1$, then $\frac{n_1 n_2}{n_1+n_2} {(\bar{X}_1-\bar{X}_2)}^T  p_1 p_1^T (\bar{X}_1-\bar{X}_2)$ distributed as $(cp+1-c)\chi^2_1$ whose variance is of order $p^2$ while $\frac{n_1 n_2}{n_1+n_2}\sum_{i=2}^p {(\bar{X}_1-\bar{X}_2)}^T  p_i p_i^T (\bar{X}_1-\bar{X}_2)$ is distributed as $(1-c)\chi^2_{p-1}$ whose variance is of order $p$. 
%The large variance is totally caused by term $p\chi^2_1$. 
%If we remove $\frac{n_1 n_2}{n_1+n_2} {(\bar{X}_1-\bar{X}_2)}^T  p_1 p_1^T (\bar{X}_1-\bar{X}_2)$ from $T_{BS}$, the variance of $T_{BS}$ can be significantly reduced to order $p$ from order $p^2$.
%Note that $\bSigma=(1-c)\BI+c\bm{1}_p+\bm{1}_p^T$ is just a special case of spiked covariance.

%In another point of view, $T_1$ is obtained by transforming $X_{k,i}$ to $\tilde{\BV}^T X_{k,i}$ ($i=1,\ldots, n_k$, $k=1,2$) and then invoking the statistic of~\cite{Chen2010A}.
Recently, a class of test procedures have been proposed through random projection to lower dimensional space. See, for example,~\cite{Lopes2015A},~\cite{Thulin2014A} and~\cite{Srivastava2014RAPTT}.
It is known that random projection based methods offer higher power when the variables are dependent.
However, these test procedures are randomized, which is undesirable in practice.
Then, is there an optimal projection which is nonrandomized?

For $\BO\in \mathbb{O}_{p\times k}$ ($k\leq p$), define statistic
$$
    T(\BO)=\|\BO^T(\bar{X}_1-\bar{X}_2)\|^2-\frac{1}{n_1}\mytr(\BO^T \BS_1\BO)-\frac{1}{n_2}\mytr(\BO^T \BS_2\BO).
$$
Then $T(\BO)$ is~\cite{Chen2010A}'s statistic on the transformed data $\BO^T X_{k,i}$.
%We would like to obtain an $\BO$ such that the associated $T(\BO)$ is most powerful.
Denote by $\Phi(\cdot)$ the cumulative distribution function of the standard normal random variable.
Under the condition~\eqref{chenscondition},~\cite{Chen2010A} proved that the asymptotic power of $T_{CQ}$ under the local alternative is
$$
\Phi\Bigg(\Phi^{-1}(\alpha)+\frac{\|\mu_1-\mu_2\|^2}{\sqrt{2\tau^2\mytr(\bSigma^2)}}\Bigg).
$$
Hence the power of $T_{CQ}$ is largely impacted by $\|\mu_1-\mu_2\|^2/\sqrt{2\tau^2\mytr(\bSigma^2)}$, which may be viewed as a signal to noise ratio (SNR).
Consequently, $\|\BO^T (\mu_1-\mu_2)\|^2/\sqrt{2\tau^2\mytr(\BO^T \bSigma^2 \BO)}$ measures the power of $T(\BO)$.
To consider an average-case scenario, like~\cite{Lopes2015A}, we temporarily place a prior on $\mu_1-\mu_2$ and assume that $\mu_1-\mu_2$ is from the uniform distribution on the unit sphere.
In this case, an average SNR can be defined as
\begin{equation}\label{touteng}
\myE \Bigg( \frac{\|\BO^T (\mu_1-\mu_2)\|^2}{\sqrt{2\tau^2\mytr(\BO^T \bSigma^2 \BO)}}\Bigg)
=  \frac{k/p}{\sqrt{2\tau^2\mytr(\BO^T \bSigma^2 \BO)}}.
\end{equation}
It can be expected that the $T(\BO)$ maximizing the average SNR has the best average power behavior among $\{T(\BO):\, \BO\in \mathbb{O}_{p\times k},\,  k\leq p\}$.

Note that for fixed $k$,~\eqref{touteng} is maximized when the columns of $\BO$ are equal to the last $k$ eigenvectors of $\bSigma$.
Thus, it remains to maximize
\begin{equation}\label{touteng2}
\frac{k/p}{\sqrt{2\tau^2\sum_{i=p-k+1}^p \lambda_i^2 (\bSigma)}}
\end{equation}
over $k$.
If $k\leq p-r$,~\eqref{touteng2} is equal to $\sqrt{k}/(\sqrt{2} \sigma^2 \tau p)$ which is an increasing function of $k$.
If $k> p-r$, we have
\begin{equation*}
    \begin{aligned}
        &\frac{k/p}{\sqrt{2\tau^2(\sum_{i=p-k+1}^r \lambda_i^2(\bSigma)+k\sigma^4)}}
        \leq
        \frac{1}{\sqrt{2\tau^2(\kappa^{-2}p^{2\beta}+(p-r)\sigma^4)}}\\
        =&
        \frac{p/(p-r)}{\sqrt{\kappa^{-2}p^{2\beta}/((p-r)\sigma^4)+1}}
\frac{(p-r)/p}{\sqrt{2\tau^2((p-r)\sigma^4)}}.
    \end{aligned}
\end{equation*}
Hence for sufficiently large $p$, we have
\begin{equation*}
    \begin{aligned}
        \frac{k/p}{\sqrt{2\tau^2(\sum_{i=p-k+1}^r \lambda_i^2(\bSigma)+k\sigma^4)}}
        <
\frac{(p-r)/p}{\sqrt{2\tau^2((p-r)\sigma^4)}},
    \end{aligned}
\end{equation*}
and~\eqref{touteng2} is maximized when $k=p-r$.
Consequently, for sufficiently large $p$,~\eqref{touteng} is maximized when $\BO=\tilde{\BV}$.

The above discussion motivates us to consider the variable
\begin{equation*}
\begin{aligned}
    T_1=\|\tilde{\BV}^T(\bar{X}_1-\bar{X}_2)\|^2-\frac{1}{n_1}\mytr(\tilde{\BV}^T \BS_1\tilde{\BV})-\frac{1}{n_2}\mytr(\tilde{\BV}^T \BS_2\tilde{\BV}).
    \\
    %&=\frac{\sum_{i\neq j}^{n_1}X_{1i}^T\tilde{\BV}\tilde{\BV}^T X_{1j}}{n_1(n_1-1)}+\frac{\sum_{i\neq j}^{n_2}X_{2i}^T\tilde{\BV}\tilde{\BV}^T X_{2j}}{n_2(n_2-1)}-2\frac{\sum_{i=1}^{n_1}\sum_{j=1}^{n_2}X_{1i}^T\tilde{\BV}\tilde{\BV}^T X_{2j}}{n_1n_2}
\end{aligned}
\end{equation*}
 Note that based on $\tilde{\BV}^T X_{ki}$, the likelihood ratio test statistic for hypothesis~\eqref{problem} is 
    $\|\tilde{\BV}^T (\bar{X}_1-\bar{X}_2)\|^2$. 
    In this view, $T_1$ can be regarded as a restricted likelihood ratio statistic.
It can be shown that $T_1$ is asymptotically normal.
\begin{proposition}\label{oracleTheorem}
    Under Assumption~\ref{theModel}, suppose $\|\mu_1-\mu_2\|^2= o(p/n)$, we have 
    \begin{equation*}
        \frac{T_1-\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2}
        {\sigma^2\sqrt{2\tau^2 p}}%\xrightarrow{\mathcal{L}}N(0,1).
        \sim \frac{\chi^2_{p-r}-(p-r)}{\sqrt{2(p-r)}}+o_P(1).
    \end{equation*}
\end{proposition}
\begin{remark}
    Proposition~\ref{oracleTheorem} implies that
    $$
        \frac{T_1-\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2}
        {\sigma^2\sqrt{2\tau^2 p}}\xrightarrow{\mathcal{L}}N(0,1).
    $$
    However, compared with the standard normal distribution, the standardized Chi-squared distribution is a better approximation of the statistic.
    This fact is implied by the proof of Proposition~\ref{oracleTheorem}.
\end{remark}







%\begin{remark}If $\BV$ is known, the model in Assumption~\ref{theModel} is very similar to random effects model. And our idea is just like REstricted Maximum Likelihood (REML).
%\end{remark}

%\begin{remark}
%    Suppose $\mu_1=\mu_2$. When $\beta>1/2$, the order of $T_1$'s variance is smaller than the order of $T_{CQ}$'s variance, which implies $T_1/(\sigma^2\sqrt{2\tau^2 p})$ is asymptotically independent of $T_{CQ}/\sqrt{2\tau^2 \mathrm{tr}\bSigma^2}$. Hence $T_1$ does provide additional information, although $T_1$ is inherited from $T_{CQ}$.  
%\end{remark} 

Note that $T_1$ % is not a statistic since it
relies on the subspace $\tilde{\BV}\tilde{\BV}^T$ which is typically unknown and thus needs to be estimated.
Let $\hat{\BV}$ and $\hat{\tilde{\BV}}$ denote the first $r$ and last $p-r$ eigenvectors of $\BS$, respectively.
\cite{Anderson1986Asymptotic} proved that
the maximum likelihood estimator (MLE) of $\BV$ is $\hat{\BV}$.
This fact, together with the equalities $\tilde{\BV}\tilde{\BV}^T= \BI_p-\BV\BV^T$ and $\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T= \BI_p-\hat{\BV}\hat{\BV}^T$, implies that 
the MLE of $\tilde{\BV}\tilde{\BV}^T$ is $\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T$.
%By replacing $\tilde{\BV}\tilde{\BV}^T$ by $\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T$, the main term $\|\tilde{\BV}^T (\bar{X}_1-\bar{X}_2)\|^2$ of $T_1$ becomes
%$$T_2=\|\hat{\tilde{\BV}}^T (\bar{X}_1-\bar{X}_2)\|^2.$$
Thus, as the main term of $T_1$,
$\|\tilde{\BV}^T (\bar{X}_1-\bar{X}_2)\|^2$ can be estimated by $\|\hat{\tilde{\BV}}^T (\bar{X}_1-\bar{X}_2)\|^2$.
In $T_1$, the centralization term $n_1^{-1}\mytr(\tilde{\BV}^T \BS_1\tilde{\BV})+n_2^{-1}\mytr(\tilde{\BV}^T \BS_2\tilde{\BV})$ is an unbiased estimator of $\tau (p-r)\sigma^2$, 
which makes $\myE T_1=0$ under the null hypothesis.
However, the centralization of $\|\hat{\tilde{\BV}}^T (\bar{X}_1-\bar{X}_2)\|^2$ is more involved.

Under the null hypothesis, 
\begin{equation*}
    \begin{aligned}
        &\myE [\|\hat{\tilde{\BV}}^T (\bar{X}_1-\bar{X}_2)\|^2|\BS]
        = \myE[ \|\hat{\tilde{\BV}}^T (\bar{X}_1-\bar{X}_2)\|^2 |\BS]
        =
\tau \mytr(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})\\
        =&
\tau (p-r) \sigma^2+
\tau \mytr(\hat{\tilde{\BV}}^T\BV \bLambda \BV^T\hat{\tilde{\BV}}).%>\tau (p-r)\sigma^2.
    \end{aligned}
\end{equation*}
Note that
$$
\begin{aligned}
    &\mytr(\hat{\tilde{\BV}}^T\BV \bLambda \BV^T\hat{\tilde{\BV}})
=
\mytr( \bLambda^{1/2} \BV^T\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T\BV \bLambda^{1/2} )
=
\mytr( \bLambda^{1/2} (\BI_r-\BV^T\hat{\BV}\hat{\BV}^T\BV) \bLambda^{1/2} )\\
    =&
    \sum_{i=1}^r  (1-\sum_{l=1}^r (\hat{v}_l^T v_i)^2)\blambda_i,
\end{aligned}
$$
where $\hat{v}_i$ and $v_i$ are the $i$th columns of $\hat{\BV}$ and $\BV$, respectively, $i=1,\ldots, r$.
Under $p=O(n\blambda_r)$ and some other regular conditions,~\cite{Fan2015Asymptotics}, Theorem 3.2 asserts that 
$$
\hat{v}_j^T v_i=O_P(\epsilon_{j,n}),\quad 1\leq i\neq j\leq r,
\quad \text{and}\quad
\hat{v}_i^T v_i=\frac{1}{\sqrt{1+ \frac{p}{n(\blambda_i+\sigma^2)}\sigma^2}}+O_P(\epsilon_{i,n}),\quad 1\leq i\leq r,
$$
where $\epsilon_{i,n}$ is higher order infinitesimal, $i=1,\ldots, r$.
This motivates us to approximate $\mytr(\hat{\tilde{\BV}}^T\BV \bLambda \BV^T\hat{\tilde{\BV}})$
by
$$
\sum_{i=1}^r \Big(1-\frac{1}{1+\frac{p}{n(\blambda_i+\sigma^2)}\sigma^2}\Big)\blambda_i
=
\sum_{i=1}^r \frac{p\sigma^2}{n\blambda_i+(n+p)\sigma^2}\blambda_i.
$$
Hence 
$$
\myE [\|\hat{\tilde{\BV}}^T (\bar{X}_1-\bar{X}_2)\|^2|\BS]
\approx
\tau(p-r)\sigma^2 + \tau\sum_{i=1}^r \frac{p\sigma^2}{n\blambda_i+(n+p)\sigma^2}\blambda_i.
$$
To centralize $\|\hat{\tilde{\BV}}^T (\bar{X}_1-\bar{X}_2)\|^2$, we need to estimate $\blambda_1,\ldots, \blambda_r$ and $\sigma^2$.
~\cite{Anderson1986Asymptotic} proved that the MLE of $\sigma^2$ is $\hat{\sigma}^2=(p-r)^{-1}\sum_{i=r+1}^p \lambda_i(\BS)$ and the MLE of $\blambda_i$ is $\lambda_i(\BS)-\hat{\sigma}^2$, $i=1,\ldots, r$.
However, Lemma~\ref{eigenconsisLemma} in Appendix~\ref{appendixB} implies that $\hat{\sigma}^2$ is downward biased and $\lambda_i(\BS)-\hat{\sigma}^2$ is upward biased.
(See also~\cite{Passemier2015} and~\cite{Fan2015Asymptotics}.)
Motivated by the results in Lemma~\ref{eigenconsisLemma}, we propose the following bias-corrected estimators:
$$
\hat{\sigma}_*^2= \Big(1-\frac{r}{n-2}\Big)^{-1} \hat{\sigma}^2,
\quad
\hat{\blambda}_i=\lambda_i(\BS)-\frac{p+n-r-2}{n-2}\hat{\sigma}_*^2
\quad i=1,\ldots,r.
$$
The following proposition gives the convergence rate of these estimators.
\begin{proposition}\label{varianceEstimation}
    Under Assumption~\ref{theModel}, we have %      $\hat{\sigma}^2$ is consistent.
    \begin{equation}\label{varianceEstimationPropResult1}
    %\hat{\sigma}^2=\sigma^2 + O_P\Big(\frac{\max (n,p)}{np}\Big),\quad
    \hat{\sigma}_*^2=\sigma^2 + O_P\Big(\max \big(\frac{1}{\sqrt{np}},\frac{1}{p}\big)\Big),
    \end{equation}
    and
    \begin{equation}\label{varianceEstimationPropResult2}
        \frac{\hat{\blambda}_i}{\blambda_i}
        =
        1+
        O_P\Big(\max\big(\frac{1}{\sqrt{n}},\frac{1}{p^{\beta}}\big)\Big).
    \end{equation}
\end{proposition}
\begin{remark}
Recently,~\cite{Passemier2015} proposed a bias-corrected estimator of $\sigma^2$:
$$
\Big(1+\frac{1}{n-2}\big(r+\hat{\sigma}^2\sum_{i=1}^r \frac{1}{\blambda_i}\big)\Big)\hat{\sigma}^2.
$$
In their paper, $\blambda_i$'s are fixed and known.
This is different from our model where $\blambda_i$'s are divergent and unknown.
Under Assumption~\ref{theModel}, we have
$$
\Big(1+\frac{1}{n-2}\big(r+\hat{\sigma}^2\sum_{i=1}^r \frac{1}{\blambda_i}\big)\Big)\hat{\sigma}^2
=
    \Big(1+\frac{r}{n-2}\Big)\hat{\sigma}^2+O_P(\frac{1}{np^{\beta}})
    =
    \hat{\sigma}_*^2
    +O_P(\frac{1}{n^2})+O_P(\frac{1}{np^{\beta}}).
$$
So the difference between~\cite{Passemier2015}'s estimator and $\hat{\sigma}_*^2$ is minor in our model.
\end{remark}
\begin{remark}
    Recently,~\cite{Fan2015Asymptotics} proposed an estimator of
    $\lambda_i(\bSigma)=\blambda_i+\sigma^2$, $i=1,\ldots, r$:
    $$
    \max\Big(\lambda_i(\BS)-\frac{p}{n-2}\Big(1-\frac{p}{p-r}\frac{r}{n-2}\Big)^{-1}\hat{\sigma}^2,0\Big).
    $$
    They showed that under $p>n-2$, $p=O(n\blambda_r)$ and some other conditions,
    $$
    \frac{1}{\blambda_i+\sigma^2}\max\Big(\lambda_i(\BS)-\frac{p}{n-2}\Big(1-\frac{p}{p-r}\frac{r}{n-2}\Big)^{-1}\hat{\sigma}^2,0\Big)
    =1+O_P\Big(\frac{1}{\blambda_i}\sqrt{\frac{p}{n}}+\frac{1}{\sqrt{n}}\Big).
    $$
    Note that under Assumption~\ref{theModel} and $p>n-2$, we have
    $$
\frac{1}{\blambda_i}\sqrt{\frac{p}{n}}+\frac{1}{\sqrt{n}}\asymp
\frac{1}{\sqrt{n}}
\asymp
        \max\big(\frac{1}{\sqrt{n}},\frac{1}{p^{\beta}}\big).
    $$
    In this case,~\cite{Fan2015Asymptotics}'s estimator and our estimator have the same convergence rate, although the estimands are slightly different.
    Compared with~\cite{Fan2015Asymptotics}'s result, Proposition~\ref{varianceEstimation} doesn't need the conditions $p>n-2$ and $p=O(n\blambda_r)$.
\end{remark}

%Similarly, we denote by  $\hat{\BV}_k$ and $\hat{\tilde{\BV}}_k$ the first $r$ and last $p-r$ eigenvectors of $S_k$ respectively, $k=1,2$.
%While ${n_k^{-1}}\mathrm{tr}(\tilde{\BV}^T S_k\tilde{\BV})$ can be estimated by ${n_k^{-1}}\mathrm{tr}(\hat{\tilde{\BV}}_k^T S_k\hat{\tilde{\BV}}_k)$, $k=1,2$.
%Define
%\begin{equation*}
%\begin{aligned}
    %T_2&=\|\hat{\tilde{\BV}}^T(\bar{X}_1-\bar{X}_2)\|^2-\frac{1}{n_1}\mathrm{tr}(\hat{\tilde{\BV}}_1^T S_1\hat{\tilde{\BV}}_1)-\frac{1}{n_2}\mathrm{tr}(\hat{\tilde{\BV}}_2^T S_2\hat{\tilde{\BV}}_2).
    %T_2=\frac{\sum_{i\neq j}^{n_1}X_{1i}^T\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T X_{1j}}{n_1(n_1-1)}+\frac{\sum_{i\neq j}^{n_2}X_{2i}^T\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T X_{2j}}{n_2(n_2-1)}
%-2\frac{\sum_{i=1}^{n_1}\sum_{j=1}^{n_2}X_{1i}^T\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T X_{2j}}{n_1n_2}
%\end{aligned}
%\end{equation*}

Now we propose the following test statistic:
$$
T_2=
\|\hat{\tilde{\BV}}^T (\bar{X}_1-\bar{X}_2)\|^2
-
\tau(p-r)\hat{\sigma}_*^2 
- \tau\sum_{i=1}^r \frac{p\hat{\sigma}_*^2}{n\hat{\blambda}_i+(n+p)\hat{\sigma}_*^2}\hat{\blambda}_i.
$$
To standardize $T_2$, note that
$$
\begin{aligned}
    &\myVar\big(
\|\hat{\tilde{\BV}}^T (\bar{X}_1-\bar{X}_2)\|^2
\big)
    =2\tau^2\big(
\mytr(\hat{\tilde{\BV}}^T \BV \bLambda \BV^T\hat{\tilde{\BV}})^2
+
2\sigma^2\mytr(\hat{\tilde{\BV}}^T \BV \bLambda \BV^T\hat{\tilde{\BV}})
+\sigma^4 (p-r)
\big)
\\
    =&
    2\tau^2\Big(
    \sum_{l=1}^r \big(1-\sum_{i=1}^r (\hat{v}_l^T v_i)^2\big)^2 \blambda_i^2
    +2\sum_{1\leq i<j \leq r}\big(\sum_{l=1}^r (\hat{v}_l^T v_i)(\hat{v}_l^T v_j )\big)^2\blambda_i\blambda_j
    \\
    &+
    2\sigma^2\sum_{l=1}^r \big(1-\sum_{i=1}^r (\hat{v}_l^T v_i)^2\big) \blambda_i
    +\sigma^4 (p-r)
    \Big)
    \\
    \approx&
    2\tau^2\Big(
    \sum_{i=1}^r \big(\frac{p\sigma^2}{n\blambda_i+(n+p)\sigma^2}\blambda_i\big)^2
    +
    2\sigma^2\sum_{i=1}^r \frac{p\sigma^2}{n\blambda_i+(n+p)\sigma^2}\blambda_i
    +\sigma^4 (p-r)
    \Big).
\end{aligned}
$$
$$
    \begin{aligned}
        Q=&T_2/
   \Bigg( 
        2\tau^2\Big(
        \sum_{i=1}^r \big(\frac{p\hat{\sigma}_*^2}{n\hat{\blambda}_i+(n+p)\hat{\sigma}_*^2}\hat{\blambda}_i\big)^2
    +
        2\sigma^2\sum_{i=1}^r \frac{p\hat{\sigma}_*^2}{n\hat{\blambda}_i+(n+p)\hat{\sigma}_*^2}\hat{\blambda}_i
        +\hat{\sigma}_*^4 (p-r)
    \Big)\Bigg)^{1/2}\\
    \end{aligned}
$$
In view of Proposition~\ref{oracleTheorem}, we reject the null hypothesis if
$$
        Q>\frac{\chi^2_{\alpha, p-r}-(p-r)}{\sqrt{2(p-r)}},
$$
where $\chi^2_{\alpha, p-r}$ is the upper $\alpha$ quantile of a $\chi^2_{p-r}$ random variable.

The following theorem establishes the asymptotic normality of $T_2$.
\begin{theorem}\label{myPanpan}
    Under Assumption~\ref{theModel},
suppose $p/n^2\to 0$ and
    $\|\mu_1-\mu_2\|^2=O({\sqrt{p}}/{n})$,
    we have
\begin{equation*}
        \frac{T_2-\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2}{\sigma^2\sqrt{2\tau^2 p}}\xrightarrow{\mathcal{L}}N(0,1).
\end{equation*}
\end{theorem} 
\begin{remark}
The asymptotic normality of $T_2$ is closely related to the convergence rate of $\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T$ to $\tilde{\BV}\tilde{\BV}^T$.
    Lemma~\ref{conRateLemma} in Appendix~\ref{appendixB} and the equality $\|\hat{\BV}\hat{\BV}^T -\BV \BV^T\|=\|\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T -\tilde{\BV} \tilde{\BV}^T\|$ imply that 
    $\|\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T -\tilde{\BV} \tilde{\BV}^T\|=O_P(p/(p^{\beta}n))$.
    Hence $\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T$ can consistently estimate $\tilde{\BV}\tilde{\BV}^T$ only if $p/(p^{\beta}n)\to 0$.
    Moreover,~\cite{Cai2012Sparse}'s Theorem $5$ implies that no other estimator has faster convergence rate than $\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T$.
    The asymptotic normality of $T_2$ requires the condition
    $$
    p^{-1}\big(p^{\beta}\|\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T -\tilde{\BV} \tilde{\BV}^T\|^2+1\big)^2\xrightarrow{P} 0.
    $$
    This is equivalent to $\|\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T -\tilde{\BV} \tilde{\BV}^T\|^2=o_P(\sqrt{p}/p^{\beta})$.
    Hence in Theorem~\ref{myPanpan} we assume $p/n^2\to 0$.
The proof of Theorem~\ref{myPanpan} implies that the asymptotic normality of $T_2$ is not valid if the condition $p/n^2\to 0$ is violated.
\end{remark}
%\begin{remark}  Compared with~\cite{2016arXiv160202491A}'s assumption (ix) which is equivalent to assuming $\frac{p^{2\beta-1}}{n_1+n_2}\to 0$ in model~\eqref{theModel}, our assumption $\frac{\sqrt{p}}{n_1+n_2}\to 0$ doesn't involved $\beta$.
%And when $\beta\geq \frac{3}{4}$, our assumption is weaker than~\cite{2016arXiv160202491A}'s. Note that when $\beta=\frac{1}{2}$, $\frac{\sqrt{p}}{n_1+n_2}$ is a necessary condition to make $\hat{\BV}\hat{\BV}^T$ a consistent
%estimator of $\BV\BV^T$ (see lemma 2 in appendix). So condition $\frac{\sqrt{p}}{n_1+n_2}$ is roughly the best we can do if the relationship between $p$ and $n$ doesn't involve $\beta$.
%\end{remark}



By Proposition~\ref{varianceEstimation},
$$
\frac{p\hat{\sigma}_*^2}{n\hat{\blambda}_i+(n+p)\hat{\sigma}_*^2}\hat{\blambda}_i\leq
\frac{p}{n}\hat{\sigma}_*^2
=o_P(p).
$$
Then it follows from Proposition~\ref{varianceEstimation} and Theorem~\ref{myPanpan} that
$$
Q=\frac{T_2}{\sigma^2\sqrt{2\tau^2 (p-r)}}(1+o_P(1))\xrightarrow{\mathcal{L}}N(0,1).
$$
Thus,
our test is asymptotically level $\alpha$.


The asymptotic power function of our test can be obtained immediately from Theorem~\ref{myPanpan}.
\begin{corollary}\label{testPowerh}
    Under the conditions of Theorem~\ref{myPanpan}, the asymptotic power function of our test is
    \begin{equation*}
        \Phi\Big(-\Phi^{-1}(1-\alpha)+\frac{\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2}{\sigma^2\sqrt{2\tau^2p}}\Big).
    \end{equation*}
\end{corollary}
From the expression of the asymptotic power function, we can see that the power of our test is not affected by the large eigenvalues of $\bSigma$.

Having derived the asymptotic power function of our test, we are now in a position to provide a comparison with other tests.
Again, to make an average-case comparison against other tests, we place a prior on $\mu_1-\mu_2$.
Suppose that the norm $\|\mu_1-\mu_2\|$ is nonrandom while the orientation $\delta=(\mu_1-\mu_2)/\|\mu_1-\mu_2\|$ is from the uniform distribution on the unit sphere.
Then $\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2=\|\mu_1-\mu_2\|^2 \|\tilde{\BV}^T\delta\|^2$.
Since $\myE \|\tilde{\BV}^T\delta\|^2=(p-r)/p$ and $\myVar \big(\|\tilde{\BV}^T\delta\|^2\big) = 2r(p-r)/\big(p^2(p+2)\big)$, we have $\|\tilde{\BV}^T\delta\|^2=1+O_P(1/p)$.
In this case, the asymptotic power function of our test is equal to
    \begin{equation*}
        \Phi\Big(-\Phi^{-1}(1-\alpha)+\frac{\|\mu_1-\mu_2\|^2}{\sigma^2\sqrt{2\tau^2p}}\Big).
    \end{equation*}
So the separation boundary of our test is $\|\mu_1-\mu_2\|^2\asymp \sqrt{p}/n$.

To compare our test with the corrected $T_{CQ}$ test,
recall that the separation boundary of the corrected $T_{CQ}$ test procedure is $\|\mu_1-\mu_2\|^2\asymp p^{\beta}/n$.
 Thus, when $\beta>1/2$, the testable region of our test is larger than that of the corrected $T_{CQ}$ test procedure.

 We would also like to compare our test with $T_{SD}$.~\cite{Srivastava2008A} showed that the asymptotic power function of $T_{SD}$ is
 $$
        \Phi\Big(-\Phi^{-1}(1-\alpha)+\frac{\|\bSigma_d^{-1/2}(\mu_1-\mu_2)\|^2}{\sqrt{2\tau^2\mytr(\BR^2)}}\Big),
 $$
 where $\bSigma_d=\mydiag(\bSigma)$ and $\BR=\bSigma_d^{-1/2} \bSigma \bSigma_d^{-1/2}$ is the population correlation matrix.
 It is known that the power of $T_{SD}$ is highest when $\bSigma$ is diagonal.

 First we consider the uniform correlation structure $\bSigma=(1-\rho)\BI_p+\rho \mathbf{1}_p\mathbf{1}_p^T$ ($0<\rho <1$) which is far away from a diagonal matrix.
 In this case, the diagonal entries of $\Sigma$ are all $1$. 
 We have
 $$
 \frac{\|\bSigma_d^{-1/2}(\mu_1-\mu_2)\|^2}{\sqrt{2\tau^2\mytr(\BR^2)}}
 =
 \frac{1}{\sqrt{\rho^2 p +(1-\rho^2)}}\frac{\|\mu_1-\mu_2\|^2}{\sqrt{2\tau^2p}}.
 $$
 Hence the asymptotic relative efficiency of our test with respect to $T_{SD}$ is
 $\sqrt{\rho^2 p +(1-\rho^2)}$.
 Our test has much higher power.

 Now we consider the diagonal covariance matrix. 
 In this case, $\bSigma=\mydiag(\blambda_1+\sigma^2,\ldots,\blambda_r+\sigma^2,\sigma^2,\ldots,\sigma^2)$ and $\BR=\BI_p$.
 Note that $\|\bSigma_d^{-1/2}(\mu_1-\mu_2)\|^2=\|\mu_1-\mu_2\|^2 \|\bSigma^{-1/2}\delta\|^2$.
 We have 
 $$\myE\|\bSigma^{-1/2}\delta\|^2=
 \frac{1}{p}\mytr\big(\bSigma^{-1}\big)
 =\sigma^{-2} (1+o_P(1))
 $$
and
 $$
 \myVar\big( \|\bSigma^{-1/2}\delta\|^2 \big)=\frac{2}{p+2}\Big(\frac{1}{p}\mytr\big(\bSigma^{-2}\big)-\big(\frac{1}{p}\mytr\big(\bSigma^{-1}\big)\big)^2\Big)=o\big(\frac{1}{p}\big).$$
 Hence $\|\bSigma^{-1/2}\delta\|^2=\sigma^{-2}(1+o_P(1))$ and the asymptotic power function of $T_{SD}$ equals to
 $$
        \Phi\Big(-\Phi^{-1}(1-\alpha)+\frac{\|\mu_1-\mu_2\|^2}{\sigma^2\sqrt{2\tau^2 p}}\Big).
 $$
 Thus, under the diagonal covariance matrix, the asymptotic power function of our test is the same as that of $T_{SD}$.






\section{Numerical studies}
\subsection{Simulation results}


In this section, we consider the simulation performance of the proposed test and compare it with several other alternatives: (1)~\cite{Chen2010A}'s test (CQ); (2)~\cite{Ma2015A}'s test (FAST); (3) the corrected $T_{CQ}$ test procedure proposed in Section~\ref{sec:chen} (CCQ); (4)~\cite{Srivastava2008A}'s test (SD); (5)~\cite{Lopes2015A}'s test (LJW).
The data generation mechanism is as follow.
We randomly choose a $\BU\in\mathbb{O}_{p\times p}$ from Haar invariant distribution.
Let $d_{i}$ equal to $p^{\beta}$ plus a random error from $U(0,1)$ (Uniform distribution between $0$ and $1$), $i=1,\ldots, r$.
Construct $p\times p$ diagonal matrix $\BD=\mydiag(\sqrt{d_1},\ldots,\sqrt{d_r}, 1,\ldots, 1)$.
Then, we independently generate data by the formula
$$
X_{k,i}=\mu_k+\BU \BD Y_{k,i}\quad \text{$i=1,\ldots,n_k$ and $k=1,2$,}
$$
where $Y_{k,i}$ is a $p$ dimensional random vector whose entries are i.i.d.\ random variables with common distribution $F$.
We will consider three different distributions of $F$.
\begin{itemize}
    \item
        Normal: $F {\sim} N(0,1)$.
    \item
        Chi-squared:  $F\sim (\chi^2_4-4)/\sqrt{8}$.
    \item
        Student's $t$: $F\sim t_4/\sqrt{2}$, where $t_4$ is a Student's $t$ random variable  with degree of freedom $4$.
\end{itemize}
Throughout the simulations, we take nominal level $\alpha=0.05$ and $r=2$.

%The key to the validation of Theorem~\ref{myPanpan} is  that $T_{\textrm{dif}}=\frac{n_1n_2|T_1-T_2|}{\sqrt{2p}(n_1+n_2)\sigma^2}$ converges to $0$.
%Here we verify it by simulation.
%We set $n_1=n_2=n$, $p=n^i$ for $i=1,2$ and plot $T_{\textrm{dif}}$ versus $p$.
%The results are illustrated in figure~\ref{fig:fig1}.
%From the results we can find that $T_{\textrm{dif}}$ clearly converges to $0$ when $p=n$.
%In the case of $p=n^2$ which is exactly beyond the assumption of Theorem~\ref{myPanpan},
%$T_{\textrm{dif}}$ is large and it's not clear whether $T_{\textrm{dif}}$  converges to $0$.
%\begin{figure}
%    \centering 
%    \includegraphics[height=6cm]{code/difference1.jpeg}
%    \includegraphics[height=6cm]{code/difference2.jpeg}\\
%    \includegraphics[height=6cm]{code/difference3.jpeg}
%    \includegraphics[height=6cm]{code/difference4.jpeg}\\
%    \includegraphics[height=6cm]{code/difference5.jpeg}
%    \includegraphics[height=6cm]{code/difference6.jpeg}\\
%    \caption{These are plots of $T_{\textrm{dif}}$ versus $p$. The first column and the second column are the case of $p=n$ and $p=n^2$, separately. The cases of $\beta=1,2,3$ are in the row $1,2,3$ separately. $r$ is set to be $3$ in all cases. }\label{fig:fig1}
%\end{figure}

In Section~\ref{methodology}, the critical value of our test is determined by the quantile of $(\chi^2_{p-r}-(p-r))/\sqrt{2(p-r)}$.
We have proved that the distribution of $Q$ is asymptotically equal to that of $(\chi^2_{p-r}-(p-r))/\sqrt{2(p-r)}$.
To compare these two distributions in the finite sample case,
Figure~\ref{} displays the Q-Q plots in different settings.
\begin{figure}
    \centering 
    \subfigure[Normal. $\beta=0.5$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[Normal. $\beta=1$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[Normal. $\beta=2$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \\
    \subfigure[Chi-squared. $\beta=0.5$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[Chi-squared. $\beta=1$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[Chi-squared. $\beta=2$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \\
    \subfigure[Student's $t$. $\beta=0.5$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[Student's $t$. $\beta=1$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[Student's $t$. $\beta=2$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \\
    \caption{Q-Q plots of the empirical distribution of $Q$ against that of $(\chi^2_{p-r}-(p-r))/\sqrt{2(p-r)}$ based on xxx independently generated $Q$. In all cases, $n_1=n_2=100$ and $p=500$.
    }\label{fig:QQ}
\end{figure}


\begin{figure}
    \centering 
    \subfigure[$n_1=n_2=50$, $p=200$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[$n_1=n_2=50$, $p=500$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[$n_1=n_2=50$, $p=800$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \\
    \subfigure[$n_1=n_2=100$, $p=200$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[$n_1=n_2=100$, $p=500$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[$n_1=n_2=100$, $p=800$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \\
    \subfigure[$n_1=n_2=150$, $p=200$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[$n_1=n_2=150$, $p=500$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[$n_1=n_2=150$, $p=800$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \caption{Q-Q plots of the empirical distribution of $Q$ against that of $(\chi^2_{p-r}-(p-r))/\sqrt{2(p-r)}$ based on xxx independently generated $Q$.
In all cases, $F$ is normal and $\beta=1$.
    }\label{fig:QQ2}
\end{figure}


Next, we consider the simulation of the empirical level. 
Samples are repeatedly generated $2000$ times to calculate empirical level.
The result is listed in Table~\ref{biaoge1}.
The empirical levels of the CQ test are larger than the nominal level in all cases, especially when $\beta$ is large.
The empirical levels of the SD test are close to the nominal level when $\beta=1/2$, but tend to be smaller than the nominal level as $\beta$ increases.
The empirical levels of the FAST test are very close to the nominal level in most cases, but tend to be smaller than the nominal level when $n_1=n_2=50$, $p=800$ and $\beta=1/2$.
The empirical levels of the CCQ test are very close to the nominal level in all cases.
Since the LJW test is exact, it's empirical levels are very close to the nominal level.
The empirical level of our test is a little inflated for $n_1=n_2=50$, but converges to the nominal level as the sample size increases.

% latex table generated in R 3.3.1 by xtable 1.8-2 package
% Sun Jul 31 01:50:20 2016
\begin{table}[ht]
\caption{$n_1=n_2=50$} 
\label{hahaha}
\vspace{3mm}
\centering
\begin{tabular}{llllllllll}
\toprule
    &   \multicolumn{3}{c}{Normal} & \multicolumn{3}{c}{Chi-squared}& \multicolumn{3}{c}{Student's $t$}  \\
    \cmidrule(r){2-4}
\cmidrule(r){5-7}
\cmidrule(r){8-10}
    $p$ & $200$ &  $500$ & $800$ & $200$  & $500$ & $800$ & $200$ & $500$ & $800$ \\ 
\midrule
    $\beta=0.5$\\
New1 & 0.1630 & 0.2535 & 0.3285 & 0.1715 & 0.2705 & 0.3590 & 0.1845 & 0.2920 & 0.3435 \\ 
New2 & 0.0800 & 0.0940 & 0.1080 & 0.0815 & 0.0905 & 0.1270 & 0.0840 & 0.1065 & 0.1085 \\ 
New3 & 0.0635 & 0.0565 & 0.0580 & 0.0610 & 0.0520 & 0.0695 & 0.0595 & 0.0640 & 0.0595 \\ 
oracle & 0.0510 & 0.0470 & 0.0520 & 0.0480 & 0.0410 & 0.0560 & 0.0520 & 0.0525 & 0.0520 \\ 
chi & 0.0565 & 0.0535 & 0.0415 & 0.0520 & 0.0515 & 0.0480 & 0.0560 & 0.0420 & 0.0445 \\ 
fast & 0.0525 & 0.0485 & 0.0290 & 0.0485 & 0.0395 & 0.0395 & 0.0560 & 0.0365 & 0.0325 \\ 
CQ & 0.0630 & 0.0645 & 0.0575 & 0.0675 & 0.0665 & 0.0750 & 0.0630 & 0.0725 & 0.0610 \\ 
SD & 0.0515 & 0.0500 & 0.0440 & 0.0525 & 0.0465 & 0.0580 & 0.0440 & 0.0515 & 0.0405 \\ 
ljw & 0.0535 & 0.0480 & 0.0455 & 0.0510 & 0.0575 & 0.0440 & 0.0440 & 0.0500 & 0.0505 \\ 
    $\beta=1$\\
New1 & 0.1700 & 0.2725 & 0.3420 & 0.1680 & 0.2730 & 0.3570 & 0.1645 & 0.2725 & 0.3450 \\ 
New2 & 0.0815 & 0.1060 & 0.1335 & 0.0900 & 0.1105 & 0.1270 & 0.0780 & 0.1080 & 0.1230 \\ 
New3 & 0.0560 & 0.0585 & 0.0620 & 0.0625 & 0.0600 & 0.0505 & 0.0505 & 0.0585 & 0.0510 \\ 
oracle & 0.0545 & 0.0560 & 0.0565 & 0.0605 & 0.0535 & 0.0485 & 0.0450 & 0.0535 & 0.0385 \\ 
chi & 0.0480 & 0.0445 & 0.0570 & 0.0490 & 0.0525 & 0.0525 & 0.0540 & 0.0400 & 0.0425 \\ 
fast & 0.0505 & 0.0470 & 0.0590 & 0.0500 & 0.0550 & 0.0530 & 0.0560 & 0.0420 & 0.0435 \\ 
CQ & 0.0710 & 0.0785 & 0.0720 & 0.0725 & 0.0775 & 0.0730 & 0.0770 & 0.0715 & 0.0675 \\ 
SD & 0.0200 & 0.0165 & 0.0130 & 0.0225 & 0.0130 & 0.0100 & 0.0215 & 0.0075 & 0.0100 \\ 
ljw & 0.0470 & 0.0520 & 0.0460 & 0.0565 & 0.0470 & 0.0535 & 0.0485 & 0.0440 & 0.0540 \\ 
    $\beta=2$\\
New1 & 0.1685 & 0.2605 & 0.3290 & 0.1750 & 0.2620 & 0.3550 & 0.1715 & 0.2620 & 0.3600 \\ 
New2 & 0.0860 & 0.1010 & 0.1215 & 0.0865 & 0.1010 & 0.1225 & 0.0795 & 0.1080 & 0.1245 \\ 
New3 & 0.0545 & 0.0535 & 0.0470 & 0.0550 & 0.0545 & 0.0585 & 0.0505 & 0.0585 & 0.0540 \\ 
oracle & 0.0535 & 0.0495 & 0.0450 & 0.0520 & 0.0545 & 0.0510 & 0.0505 & 0.0440 & 0.0450 \\ 
chi & 0.0510 & 0.0505 & 0.0495 & 0.0480 & 0.0540 & 0.0530 & 0.0485 & 0.0410 & 0.0475 \\ 
fast & 0.0540 & 0.0530 & 0.0510 & 0.0500 & 0.0565 & 0.0555 & 0.0520 & 0.0445 & 0.0495 \\ 
CQ & 0.0690 & 0.0760 & 0.0675 & 0.0745 & 0.0665 & 0.0630 & 0.0785 & 0.0745 & 0.0700 \\ 
SD & 0.0060 & 0.0010 & 0.0000 & 0.0045 & 0.0010 & 0.0010 & 0.0055 & 0.0010 & 0.0010 \\ 
ljw & 0.0435 & 0.0500 & 0.0535 & 0.0410 & 0.0450 & 0.0430 & 0.0530 & 0.0500 & 0.0515 \\ 
\bottomrule
\end{tabular}
\end{table}


\begin{table}[ht]
\caption{$n_1=n_2=100$} 
\label{hahaha}
\vspace{3mm}
\centering
\begin{tabular}{llllllllll}
\toprule
    &   \multicolumn{3}{c}{Normal} & \multicolumn{3}{c}{Chi-squared}& \multicolumn{3}{c}{Student's $t$}  \\
    \cmidrule(r){2-4}
\cmidrule(r){5-7}
\cmidrule(r){8-10}
    $p$ & $200$ &  $500$ & $800$ & $200$  & $500$ & $800$ & $200$ & $500$ & $800$ \\ 
\midrule
    $\beta=0.5$\\
New1 & 0.0960 & 0.1360 & 0.1560 & 0.1075 & 0.1255 & 0.1640 & 0.1135 & 0.1410 & 0.1660 \\ 
New2 & 0.0600 & 0.0670 & 0.0770 & 0.0685 & 0.0705 & 0.0790 & 0.0660 & 0.0710 & 0.0760 \\ 
New3 & 0.0515 & 0.0470 & 0.0550 & 0.0595 & 0.0535 & 0.0535 & 0.0545 & 0.0565 & 0.0515 \\ 
oracle & 0.0475 & 0.0455 & 0.0525 & 0.0555 & 0.0495 & 0.0470 & 0.0535 & 0.0480 & 0.0490 \\ 
chi & 0.0480 & 0.0475 & 0.0475 & 0.0600 & 0.0510 & 0.0380 & 0.0430 & 0.0430 & 0.0450 \\ 
fast & 0.0510 & 0.0505 & 0.0475 & 0.0660 & 0.0555 & 0.0390 & 0.0480 & 0.0465 & 0.0460 \\ 
CQ & 0.0680 & 0.0610 & 0.0640 & 0.0650 & 0.0575 & 0.0630 & 0.0585 & 0.0705 & 0.0510 \\ 
SD & 0.0580 & 0.0520 & 0.0535 & 0.0560 & 0.0500 & 0.0485 & 0.0410 & 0.0615 & 0.0445 \\ 
ljw & 0.0440 & 0.0440 & 0.0460 & 0.0495 & 0.0470 & 0.0475 & 0.0440 & 0.0450 & 0.0505 \\ 
    $\beta=1$\\
New1 & 0.0975 & 0.1410 & 0.1600 & 0.0885 & 0.1320 & 0.1690 & 0.0895 & 0.1220 & 0.1625 \\ 
New2 & 0.0645 & 0.0690 & 0.0800 & 0.0545 & 0.0735 & 0.0770 & 0.0585 & 0.0655 & 0.0795 \\ 
New3 & 0.0530 & 0.0500 & 0.0475 & 0.0445 & 0.0545 & 0.0510 & 0.0455 & 0.0475 & 0.0500 \\ 
oracle & 0.0560 & 0.0495 & 0.0415 & 0.0425 & 0.0565 & 0.0455 & 0.0465 & 0.0465 & 0.0490 \\ 
chi & 0.0545 & 0.0480 & 0.0530 & 0.0500 & 0.0370 & 0.0400 & 0.0460 & 0.0405 & 0.0530 \\ 
fast & 0.0550 & 0.0500 & 0.0550 & 0.0525 & 0.0415 & 0.0450 & 0.0465 & 0.0450 & 0.0555 \\ 
CQ & 0.0715 & 0.0665 & 0.0880 & 0.0655 & 0.0680 & 0.0730 & 0.0785 & 0.0655 & 0.0740 \\ 
SD & 0.0220 & 0.0135 & 0.0085 & 0.0205 & 0.0140 & 0.0095 & 0.0225 & 0.0115 & 0.0100 \\ 
ljw & 0.0525 & 0.0515 & 0.0485 & 0.0420 & 0.0475 & 0.0475 & 0.0485 & 0.0445 & 0.0465 \\ 
    $\beta=2$\\
New1 & 0.1000 & 0.1285 & 0.1690 & 0.1025 & 0.1260 & 0.1635 & 0.0995 & 0.1215 & 0.1725 \\ 
New2 & 0.0620 & 0.0740 & 0.0815 & 0.0570 & 0.0730 & 0.0775 & 0.0630 & 0.0670 & 0.0800 \\ 
New3 & 0.0510 & 0.0500 & 0.0565 & 0.0470 & 0.0525 & 0.0445 & 0.0510 & 0.0465 & 0.0495 \\ 
oracle & 0.0475 & 0.0505 & 0.0520 & 0.0425 & 0.0505 & 0.0450 & 0.0505 & 0.0475 & 0.0525 \\ 
chi & 0.0420 & 0.0605 & 0.0530 & 0.0545 & 0.0445 & 0.0485 & 0.0615 & 0.0490 & 0.0430 \\ 
fast & 0.0460 & 0.0635 & 0.0580 & 0.0565 & 0.0475 & 0.0525 & 0.0640 & 0.0520 & 0.0455 \\ 
CQ & 0.0855 & 0.0775 & 0.0675 & 0.0615 & 0.0695 & 0.0755 & 0.0750 & 0.0740 & 0.0700 \\ 
SD & 0.0055 & 0.0025 & 0.0015 & 0.0030 & 0.0000 & 0.0005 & 0.0025 & 0.0005 & 0.0005 \\ 
ljw & 0.0480 & 0.0495 & 0.0545 & 0.0410 & 0.0455 & 0.0510 & 0.0535 & 0.0435 & 0.0550 \\ 
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{$n_1=n_2=150$} 
\label{hahaha}
\vspace{3mm}
\centering
\begin{tabular}{llllllllll}
\toprule
    &   \multicolumn{3}{c}{Normal} & \multicolumn{3}{c}{Chi-squared}& \multicolumn{3}{c}{Student's $t$}  \\
    \cmidrule(r){2-4}
\cmidrule(r){5-7}
\cmidrule(r){8-10}
    $p$ & $200$ &  $500$ & $800$ & $200$  & $500$ & $800$ & $200$ & $500$ & $800$ \\ 
\midrule
    $\beta=0.5$\\
New1 & 0.0825 & 0.1000 & 0.1125 & 0.0830 & 0.1045 & 0.1115 & 0.0875 & 0.1095 & 0.1115 \\ 
New2 & 0.0580 & 0.0575 & 0.0675 & 0.0625 & 0.0635 & 0.0655 & 0.0555 & 0.0650 & 0.0650 \\ 
New3 & 0.0520 & 0.0495 & 0.0575 & 0.0560 & 0.0505 & 0.0540 & 0.0495 & 0.0540 & 0.0540 \\ 
oracle & 0.0500 & 0.0450 & 0.0515 & 0.0550 & 0.0510 & 0.0505 & 0.0475 & 0.0520 & 0.0490 \\ 
chi & 0.0475 & 0.0545 & 0.0485 & 0.0450 & 0.0415 & 0.0480 & 0.0510 & 0.0570 & 0.0440 \\ 
fast & 0.0510 & 0.0595 & 0.0510 & 0.0510 & 0.0440 & 0.0530 & 0.0625 & 0.0645 & 0.0495 \\ 
CQ & 0.0655 & 0.0780 & 0.0710 & 0.0720 & 0.0515 & 0.0635 & 0.0565 & 0.0725 & 0.0660 \\ 
SD & 0.0530 & 0.0675 & 0.0600 & 0.0585 & 0.0445 & 0.0585 & 0.0465 & 0.0600 & 0.0560 \\ 
ljw & 0.0540 & 0.0520 & 0.0535 & 0.0545 & 0.0555 & 0.0480 & 0.0470 & 0.0500 & 0.0605 \\ 
    $\beta=1$\\
New1 & 0.0780 & 0.1000 & 0.1040 & 0.0845 & 0.0890 & 0.1070 & 0.0775 & 0.0910 & 0.1100 \\ 
New2 & 0.0555 & 0.0615 & 0.0685 & 0.0595 & 0.0570 & 0.0585 & 0.0560 & 0.0620 & 0.0610 \\ 
New3 & 0.0490 & 0.0520 & 0.0490 & 0.0515 & 0.0465 & 0.0425 & 0.0470 & 0.0520 & 0.0460 \\ 
oracle & 0.0485 & 0.0485 & 0.0545 & 0.0480 & 0.0500 & 0.0395 & 0.0475 & 0.0480 & 0.0420 \\ 
chi & 0.0405 & 0.0505 & 0.0435 & 0.0470 & 0.0445 & 0.0515 & 0.0510 & 0.0490 & 0.0450 \\ 
fast & 0.0430 & 0.0530 & 0.0455 & 0.0495 & 0.0480 & 0.0530 & 0.0545 & 0.0510 & 0.0470 \\ 
CQ & 0.0785 & 0.0690 & 0.0740 & 0.0720 & 0.0695 & 0.0715 & 0.0730 & 0.0795 & 0.0725 \\ 
SD & 0.0265 & 0.0115 & 0.0105 & 0.0220 & 0.0130 & 0.0090 & 0.0145 & 0.0145 & 0.0080 \\ 
ljw & 0.0430 & 0.0605 & 0.0460 & 0.0515 & 0.0580 & 0.0525 & 0.0580 & 0.0595 & 0.0510 \\ 
    $\beta=2$\\
New1 & 0.0775 & 0.0895 & 0.1165 & 0.0840 & 0.0995 & 0.1235 & 0.0940 & 0.1035 & 0.1145 \\ 
New2 & 0.0550 & 0.0590 & 0.0735 & 0.0590 & 0.0595 & 0.0740 & 0.0675 & 0.0645 & 0.0655 \\ 
New3 & 0.0495 & 0.0495 & 0.0565 & 0.0510 & 0.0490 & 0.0560 & 0.0575 & 0.0520 & 0.0520 \\ 
oracle & 0.0465 & 0.0485 & 0.0555 & 0.0505 & 0.0485 & 0.0485 & 0.0555 & 0.0505 & 0.0475 \\ 
chi & 0.0495 & 0.0405 & 0.0480 & 0.0460 & 0.0530 & 0.0545 & 0.0475 & 0.0395 & 0.0510 \\ 
fast & 0.0525 & 0.0415 & 0.0505 & 0.0490 & 0.0560 & 0.0560 & 0.0490 & 0.0400 & 0.0555 \\ 
CQ & 0.0675 & 0.0705 & 0.0695 & 0.0765 & 0.0655 & 0.0775 & 0.0785 & 0.0720 & 0.0875 \\ 
SD & 0.0040 & 0.0020 & 0.0005 & 0.0030 & 0.0000 & 0.0000 & 0.0025 & 0.0010 & 0.0000 \\ 
ljw & 0.0490 & 0.0475 & 0.0490 & 0.0465 & 0.0535 & 0.0480 & 0.0490 & 0.0445 & 0.0490 \\ 
\bottomrule
\end{tabular}
\end{table}





Now we consider the simulation of the empirical power.
In view of Corollary~\ref{testPowerh}, we define SNR as $\textrm{SNR}=\|\mu_1-\mu_2\|^2/(\sigma^2\sqrt{2\tau^2 p})$.
We take $\mu_1=\mathbf{0}_p$.
The orientation of $\mu_2$ is from the uniform distribution on the unit sphere.
The norm of $\mu_2$ is selected to make SNR equal to specific values.
Samples are repeatedly generated $2000$ times to calculate empirical power.
The simulation results are illustrated in figure~\ref{fig:fig1}.
From the results, we can find that when $\bSigma$ is spiked, the new test outperforms $T_{CQ}$ substantially; when $\bSigma$ is not spiked, all three tests have similar performance.
\begin{figure}
    \centering 
    \subfigure[Normal. $\beta=0.5$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[Normal. $\beta=1$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[Normal. $\beta=2$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \\
    \subfigure[Chi-squared. $\beta=0.5$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[Chi-squared. $\beta=1$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[Chi-squared. $\beta=2$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \\
    \subfigure[Student's $t$. $\beta=0.5$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[Student's $t$. $\beta=1$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[Student's $t$. $\beta=2$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \\
    \caption{Empirical power simulation. In all cases, $n_1=n_2=100$ and $p=500$.
    }\label{fig:Power}
\end{figure}

\begin{figure}
    \centering 
    \subfigure[$n_1=n_2=50$, $p=200$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[$n_1=n_2=50$, $p=500$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[$n_1=n_2=50$, $p=800$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \\
    \subfigure[$n_1=n_2=100$, $p=200$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[$n_1=n_2=100$, $p=500$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[$n_1=n_2=100$, $p=800$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \\
    \subfigure[$n_1=n_2=150$, $p=200$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[$n_1=n_2=150$, $p=500$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \subfigure[$n_1=n_2=150$, $p=800$.]{
        \includegraphics[width=0.3\textwidth]{figure/1QQPLOT1}
    }
    \caption{Empirical power simulation. In all cases, $F$ is normal and $\beta=1$.}\label{fig:Power2}
\end{figure}

%Permutation method is computation expensive. So when $p$ and $n$ are large, we simulate empirical power by asymptotic distribution. The results are illustrated in figure~\eqref{fig:fig3}.

%\begin{figure}\label{fig:fig3}
    %\centering 
    %\includegraphics[height=6cm]{code/newfig1.jpeg}
    %\includegraphics[height=6cm]{code/newfig2.jpeg}
    %\\
    %\includegraphics[height=6cm]{code/newfig3.jpeg}
    %\includegraphics[height=6cm]{code/newfig4.jpeg}
    %\\
    %\includegraphics[height=6cm]{code/newfig5.jpeg}
    %\includegraphics[height=6cm]{code/newfig6.jpeg}
    %\caption{Empirical Power (critical values are computed by asymptotic distribution)}\label{fig:fig3}
%\end{figure}

\subsection{Real data analysis}
In this section, we study the practical problem considered in~\cite{Ma2015A}.
The task is to test whether Monday stock returns are equal to those of other trading days on average.
Define an observation be the log return of stocks in a day.
Hence $p$ is the total number of stocks.
Let sample $1$ and sample $2$ be the observations on Monday and the other trading days, respectively.
Then we would like to test $H_0\, :\mu_1=\mu_2$ v.s. $H_1\,:\mu_1\neq \mu_2$.
We collected the data of $p=710$
 stocks of China
from 01/04/2013 to 12/31/2014. There are total $n_1=95$ Monday and $n_2=388$ other trading days. 

We assume $\bSigma_1=\bSigma_2$.
The first eigenvalue of $\BS$ is $0.14$, which is significantly larger than the others.
In fact, the second eigenvalue is $0.02$.
Hence there's clearly a spiked eigenvalue.
We take $r=1$ and perform our new test.
The $p$ value is $0.149$, which is obtained by permutation method with $1000$ permutations.
Hence, the null hypothesis can not be rejected for $\alpha=0.05$.
We draw the same conclusion as~\cite{Ma2015A}.

\section{Conclusion remark}



This paper is concerned with the problem of testing the equality of means in the setting of high dimension and spiked covariance.
We derived the asymptotic distribution of~\cite{Chen2010A}'s test statistic.
To reduce the variance of $T_{CQ}$, we dropped big variance terms and obtain a new test statistic. The asymptotic normality of the new statistic is proved and the asymptotic power is given. %The new test outperforms $T_{CQ}$ substantially if the variance is spiked.

%We also generalize the test to unequal variance case.

In another paper,~\cite{Zhao2016A} proved that their test statistic can be written in the form of projection. Their simulation results showed that their test performs well under strong correlations.
Our work partially explains why their test performs well although the projections are slightly different. 

 Spiked covariance is an important correlation pattern and has been widely studied in terms of PCA\@.
 In PCA, authors focus on the principal subspace.
 However, in some circumstances, as our work have shown, the complement of principal subspace is more useful. 

In our paper, we have assumed $r$ is known. If $r$ is an unknown positive number, a consistent estimator of $r$ is
\begin{equation}\label{estimateR}
    \hat{r}=\textrm{argmax}_{l\leq R}\frac{\lambda_l(\BS)}{\lambda_{l+1}(\BS)},
\end{equation}
where $R$ is a hyperparameter.
    See~\cite{Ahn2009Eigenvalue} for detail.


The asymptotic normality of the new test statistic relies on the assumption $\sqrt{p}/n\to 0$. In the situation of small $n$ or very large $p$, the critical value of the new test can be determined by permutation method. Our simulation shows that the new test still performs well. It remains a theoretical interest to study the asymptotic behavior of permutation based test in these situations.

{\color{red}
Non normality
}


%We denote by $\|\cdot \|$ and $\|\cdot\|_F$ the operator and Frobenius  norm of matrix, respectively.

%\begin{lemma}\label{lemma1}
%    let $X$ be a $p$-dimensional random vector with distribution $N(0,\bSigma)$. Denote the spectral decomposition of $\bSigma$ by $\bSigma =\sum_{i=1}^p \lambda_i p_i p_i^T$ with $\lambda_1\geq \cdots \geq \lambda_p$. Then $X^T p_i p_i^T X$ is stochastically larger than $X^T p_j p_{j}^T X$ for $i<j$.
%\end{lemma}
%\begin{proof}[\textbf{Proof}]
%    The lemma is established immediately once we note that $X^T p_i p_i^T X/\sqrt{\lambda_i}$ is distributed as $\chi^2$ distribution with freedom $1$.
%\end{proof}

\begin{appendices}
    \section{Proofs of the results in Section~\ref{sec:chen}}\label{appendixA}
\begin{proof}[\textbf{Proof of Lemma~\ref{quadraticFormCLT}}]
    By a standard orthogonal transformation, we can write
    \begin{equation}
        \frac{Y_n^T \BA_n Y_n-\myE Y_n^T \BA_n Y_n}{{[\myVar(Y_n^T \BA_n Y_n)]}^{1/2}}=\sum_{i=1}^{k_n}\frac{\lambda_i(\BA_n)}{{\big[2\mytr(\BA_n^2)\big]}^{1/2}}(Z_{ni}^2-1),
    \end{equation}
    where $Z_{n1},\ldots,Z_{n k_n}$ are independent standard normal random variables.

    If~\ref{quadraticEigen} holds, then
    \begin{equation*}
        \begin{aligned}
            &\sum_{i=1}^{k_n}\myE\Big[\frac{\lambda_i^2(\BA_n)}{2\mathrm{tr}(\BA_n^2)}{(Z_{ni}^2-1)}^2\Big\{\frac{\lambda_i^2(\BA_n)}{2\mathrm{tr}(\BA_n^2)}{(Z_{ni}^2-1)}^2\geq \epsilon\Big\}\Big]\\
            \leq&\sum_{i=1}^{k_n}
            \frac{\lambda_i^2(\BA_n)}{2\mytr(\BA_n^2)}
            \myE\Big[{(Z_{n1}^2-1)}^2\Big\{\frac{\lambda_{1}(\BA_n^2)}{2\mytr(\BA_n^2)}{(Z_{n1}^2-1)}^2\geq \epsilon\Big\}\Big]\\
            =&
            \frac{1}{2}\myE\Big[{(Z_{n1}^2-1)}^2\Big\{\frac{\lambda_{1}(\BA_n^2)}{2\mytr(\BA_n^2)}{(Z_{n1}^2-1)}^2\geq \epsilon\Big\}\Big]\to 0.
        \end{aligned}
    \end{equation*}
    Hence~\ref{quadratic} follows by Lindeberg's central limit theorem.

    Conversely, if~\ref{quadratic} holds, we will prove that there is a subsequence of $\{n\}$ along which~\ref{quadraticEigen} holds. Then~\ref{quadraticEigen} follows by a standard contradiction argument. 

    Denote $c_{ni}=\lambda_i(\BA_n)/{\big[2\mytr(\BA_n^2)\big]}^{1/2}$, $i=1,\ldots,k_n$.
    Since~\ref{quadratic} holds, the characteristic function of
        $
        \sum_{i=1}^{k_n}c_{ni}(Z_{ni}^2-1)
    $
    converges to $\exp(-t^2/2)$ for every $t$.
    Denote by $\log z$ $(z\in\mathbb{C})$ the principal branch of the complex logarithm.
    For $t\in (-1/2,1/2)$, we have
    \begin{equation*}
        \begin{aligned}
            &\myE\big[\exp{\big(it \sum_{j=1}^{k_n}c_{nj}(Z_{nj}^2-1)\big)}\big]
            =
            \exp\Big(-i(\sum_{j=1}^{k_n}c_{nj})t-
            \frac{1}{2}\sum_{j=1}^{k_n}\log(1-i2c_{nj}t) \Big)\\
            =&
            \exp\Big(
            -i(\sum_{j=1}^{k_n}c_{nj})t+
            \frac{1}{2}\sum_{j=1}^{k_n}\sum_{l=1}^{+\infty}\frac{1}{l}{(i2c_{nj}t)}^l
            \Big)
            =
            \exp\Big(
            -i(\sum_{j=1}^{k_n}c_{nj})t+
            \frac{1}{2}\sum_{l=1}^{+\infty}\Big[\sum_{j=1}^{k_n}{(c_{nj})}^l\Big]\frac{1}{l}{(i2t)}^l\Big)\\
            =&
            \exp\Big(
            -\frac{1}{2}t^2+
            \frac{1}{2}\sum_{l=3}^{+\infty}\Big[\sum_{j=1}^{k_n}{(c_{nj})}^l\Big]\frac{1}{l}{(i2t)}^l \Big),
        \end{aligned}
    \end{equation*}
    where the second equality holds since $0\leq c_{ni}\leq \sqrt{2}/2$ by definition.
    Let $b_{nl}=\sum_{j=1}^{k_n}{(c_{nj})}^l$, $n=1,2,\cdots$ and $l=3,4,\cdots$.
    Note that for $l\geq 3$, we have
    $$|b_{nl}|=\big|\sum_{j=1}^{k_n}{(c_{nj})}^l\big|\leq \big|\sum_{j=1}^{k_n}{(c_{nj})}^2\big|=1/2.$$
    By Helly's selection theorem, there's a subsequence of $\{n\}$ along which $\lim_{n\to \infty}b_{nl}=b_l$ exists for every $l$.
    For this subsequence, applying dominated convergence theorem yields
            $$\myE \big[\exp{\big(it \sum_{j=1}^{k_n}c_{nj}(Z_{nj}^2-1)\big)}\big]
            \to
            \exp\Big(
            -\frac{1}{2}t^2+
            \frac{1}{2}\sum_{l=3}^{+\infty}b_l\frac{1}{l}{(i2t)}^l\Big),
            \quad t\in\big(-\frac{1}{2},\frac{1}{2}\big).
            $$
            But the left hand side converges to $\exp(-t^2/2)$.
            It follows that
            $$
            -\frac{1}{2}t^2+
            \frac{1}{2}\sum_{l=3}^{+\infty}b_l\frac{1}{l}{(i2t)}^l
            =-\frac{1}{2}t^2+ 2\pi m i,
            \quad t\in\big(-\frac{1}{2},\frac{1}{2}\big),
            $$
            for some integer $m$.
            By the uniqueness of power series, we mulst have $m=0$ and $b_l=0$ for $l\geq 3$. Then~\ref{quadraticEigen} follows by noting that $b_{n4}\geq \max_j{(c_{nj})}^4$.
\end{proof}





\begin{proof}[\textbf{Proofs of Theorems~\ref{Chenstheory1} and~\ref{Chenstheory2}}]
    In both Theorems, (a) is a corrolary of (b).
    We shall prove (b) of Theorems~\ref{Chenstheory1} and~\ref{Chenstheory2} simultaneously.
    
    Note that $(n_k-1)\BS_k\sim \text{Wishart}_p(n_k-1,\bSigma)$, $k=1,2$,
    where $\text{Wishart}_p(m,\Psi)$ is the $p$ dimensional Wishart distribution with parameter $\Psi$ and $m$ degrees of freedom.
    We have %$(n_1-1)\mytr S_1\sim \sum_{i=1}^{p} \lambda_i (\bSigma) W_i$ where $W_i\sim \chi^2_{n_1-1} $.
    %Note that $\lambda_{r+1}(\bSigma)=\cdots=\lambda_p(\bSigma)=\sigma^2$, by central limit theorem, we have
    %$$
    %\begin{aligned}
        %&\sum_{i=1}^{p} \lambda_i (\bSigma) W_i=
    %\sum_{i=1}^{r} \lambda_i (\bSigma) W_i+
    %\sum_{i=r+1}^{p} \sigma^2 W_i\\
        %\sim&
    %\sum_{i=1}^{r} \lambda_i (\bSigma) W_i+
        %(p-r)(n_1-1)\sigma^2+
        %\sqrt{(n_1-1)(p-r)}\sigma^2\epsilon+o_P(\sqrt{np}),
    %\end{aligned}
    %$$
    %where $\epsilon\sim N(0,1)$. By law of large numbers, we have $W_i/(n_i-1)\xrightarrow{P}1$. Hence
%$$
%\begin{aligned} 
    %\frac{1}{\tau p^{\beta} n_1}\mytr S_1
    %&\sim
    %\frac{1}{\tau n_1}\sum_{i=1}^{r} \frac{\lambda_i +\sigma^2}{p^{\beta}} \frac{W_i}{n_i-1}+
        %\frac{p-r}{\tau p^{\beta}n_1}\sigma^2+
        %\frac{1}{\tau p^\beta n_1}\sqrt{\frac{p-r}{n_1-1}}\sigma^2\epsilon+o_P(n^{-1/2}p^{1/2-\beta})\\
        %&=
    %\frac{1}{\tau n_1}\sum_{i=1}^{r} l_i+
        %\frac{p-r}{\tau p^{\beta}n_1}\sigma^2+
        %\frac{1}{\tau p^\beta n_1}\sqrt{\frac{p-r}{n_1-1}}\sigma^2\epsilon+o_P(1)\\
%\end{aligned}
%$$
%
    %Hence
    $$
 \myE\Big(\frac{1}{n_1}\mytr \BS_1+\frac{1}{n_2}\mytr \BS_2\Big)=\tau \mytr\bSigma,
    $$
    and
    $$
    \begin{aligned}
        &\myVar\Big(\frac{1}{n_1}\mytr \BS_1+\frac{1}{n_2}\mytr \BS_2\Big)=
        \Big(\frac{2}{n_1^2(n_1-1)}+\frac{2}{n_2^2(n_2-1)}\Big)\mytr \bSigma^2\\
        =&
    O\Big(\frac{1}{n^3}(p^{2\beta}+p)\Big)=O\Big(\frac{p^{2\beta}}{n^3}\Big).
    \end{aligned}
    $$
    It follows that
    $$
    \begin{aligned}
        &\frac{1}{n_1}\mytr \BS_1+\frac{1}{n_2}\mytr \BS_2=
    \tau \mytr \bSigma+O_P\Big(\frac{1}{n\sqrt{n}}p^{\beta}\Big)\\
        =&\tau \sum_{i=1}^r (\blambda_i+\sigma^2)+\tau(p-r)\sigma^2+O_P\Big(\frac{1}{n\sqrt{n}}p^{\beta}\Big)\\
        =&\tau p^{\beta} \sum_{i=1}^r \omega_i+\tau(p-r)\sigma^2+o_P\Big(\frac{1}{n}p^{\beta}\Big).
    \end{aligned}
    $$
Thus,
        \begin{equation}\label{eq:kkk1}
        \frac{1}{\tau p^\beta}\big(\frac{1}{n_1}\mytr \BS_1+\frac{1}{n_2}\mytr \BS_2\big)
        =\sum_{i=1}^r \omega_i+p^{1-\beta}\sigma^2+o_P(1).
        \end{equation}

    Next we deal with $\|\bar{X}_1-\bar{X}_2\|^2$.
    Note that we have
    $$
    \|\bar{X}_1-\bar{X}_2\|^2=
    \|\BV^T(\bar{X}_1-\bar{X}_2)\|^2+
    \|\tilde{\BV}^T(\bar{X}_1-\bar{X}_2)\|^2.
    $$
    These two terms are independent.
    For the first term, note that $\BV^T(\bar{X}_1-\bar{X}_2)\sim N_r\big(\BV^T (\mu_1-\mu_2),\tau (\bLambda+\sigma^2 \BI_r)\big)$, we have
    $$
    \begin{aligned}
        \|\BV^T(\bar{X}_1-\bar{X}_2)\|^2&\sim
        \sum_{i=1}^r \Big(\sqrt{\tau (\lambda_i+\sigma^2)}Z_i+\big(\BV^T (\mu_1-\mu_2)\big)_i \Big)^2\\
        &=\tau p^{\beta}
        \sum_{i=1}^r
        \Big( \sqrt{p^{-\beta}(\lambda_i+\sigma^2)}Z_i+\frac{1}{\sqrt{\tau p^{\beta}}}\big(\BV^T (\mu_1-\mu_2)\big)_i \Big)^2.
    \end{aligned}
    $$
    By the assumptions of the theorem,  we have that
    \begin{equation}\label{eq:kkk2}
    \begin{aligned}
        \frac{1}{\tau p^{\beta}}\|\BV^T(\bar{X}_1-\bar{X}_2)\|^2
        \xrightarrow{w}
        \sum_{i=1}^r (\sqrt{\omega_i} Z_i+\zeta_i)^2.
    \end{aligned}
    \end{equation}

    As for $\|\tilde{\BV}^T(\bar{X}_1-\bar{X}_2)\|^2$, we have that
        $$
        \begin{aligned}
            &\|\tilde{\BV}^T(\bar{X}_1-\bar{X}_2)\|^2
            =\big\|\tilde{\BV}^T(\mu_1-\mu_2)+\tilde{\BV}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)\big\|^2\\
            =&\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2+
            \big\|\tilde{\BV}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)\big\|^2+
            2{(\mu_1-\mu_2)}^T\tilde{\BV}\tilde{\BV}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big).
        \end{aligned}
        $$
Since $\tilde{\BV}^T (\bar{X}_1-\bar{X}_2)\sim N_{p-r}(\tilde{\BV}^T (\mu_1-\mu_2),  \sigma^2 \tau \BI_{p-r})$, by central limit theorem, we have
    $$
\frac{
    \big\|\tilde{\BV}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)\big\|^2-\sigma^2 \tau (p-r)}{\sigma^2 \tau\sqrt{2(p-r)}}\xrightarrow{\mathcal{L}} N(0,1).
    $$
    For the intersection term, we have
    \begin{equation*}
        \begin{aligned}
            &2{(\mu_1-\mu_2)}^T\tilde{\BV}\tilde{\BV}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)
            \sim N(0,4\sigma^2 \tau \|\tilde{\BV}^T(\mu_1-\mu_2)\|^2)\\
            =& O_P(\sqrt{\tau}\|\tilde{\BV}^T(\mu_1-\mu_2)\| )=o_P(\tau p^{\beta}).
        \end{aligned}
    \end{equation*}
    It follows that
    \begin{equation}\label{eq:kkk3}
\frac{1}{\tau p^\beta}
    \big(\big\|\tilde{\BV}^T(\bar{X}_1-\bar{X}_2)\big\|^2-\sigma^2 \tau (p-r)-\big\|\tilde{\BV}^T(\mu_1-\mu_2)\big\|^2\big)
    \xrightarrow{\mathcal{L}} 
        \sqrt{2}\sigma^2 \delta_{\{\frac{1}{2}\}}(\beta)Z_0,
    \end{equation}
    where $\delta_{\frac{1}{2}}(\beta)$ equals $1$ if $\beta=1/2$ and equals $0$ otherwise.


    Combining~\eqref{eq:kkk1}~\eqref{eq:kkk2} and~\eqref{eq:kkk3} leads to
    $$
    \begin{aligned}
        &\frac{1}{\tau p^{\beta}} T_{CQ}
        =\frac{1}{\tau p^{\beta}}\big(\|\bar{X}_1-\bar{X}_2\|^2-\frac{1}{n_1}\mytr \BS_1-\frac{1}{n_2}\mytr \BS_2\big)\\
        =&
        \frac{1}{\tau p^{\beta}}{\|\BV^T(\bar{X}_1-\bar{X}_2)\|^2}+
        \frac{1}{\tau p^{\beta}} \big({\|\tilde{\BV}^T(\bar{X}_1-\bar{X}_2)\|^2-\sigma^2 \tau(p-r)-\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2}\big)\\
        &-\frac{1}{\tau p^{\beta}}\Big(\frac{1}{n_1}\mytr \BS_1+\frac{1}{n_2}\mytr \BS_2\Big)+\frac{\sigma^2 (p-r)}{p^\beta}+\frac{1}{\tau p^\beta}\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2\\
        =&
        \sum_{i=1}^r (\sqrt{\omega_i} Z_i+\zeta_i)^2+
   \sqrt{2} \sigma^2 \delta_{\{\frac{1}{2}\}}(\beta)Z_0
        -
        (\sum_{i=1}^r \omega_i+p^{1-\beta}\sigma^2)
        +\frac{\sigma^2 (p-r)}{p^\beta}+\zeta^*+o_P(1)\\
        \xrightarrow{\mathcal{L}}&
        \sum_{i=1}^r (\sqrt{\omega_i} Z_i+\zeta_i)^2+
\zeta^*+
    \sqrt{2}\sigma^2 \delta_{\{\frac{1}{2}\}}(\beta)Z_0
        -
        \sum_{i=1}^r \omega_i.
    \end{aligned}
    $$
    This implies the conclusions of Theorem~\ref{Chenstheory1} and Theorem~\ref{Chenstheory2}.

    %Note that we have $\|\bar{X}_1-\bar{X}_2\|^2\sim \tau(\sum_{i=1}^r \lambda_i(\bSigma)Z_i+\sigma^2 W)$, where $Z_i\overset{i.i.d.}{\sim}\chi^2_1$ and $W\sim \chi^2_{p-r}$ is independent of $Z_i$'s. Then
    %$$
    %\frac{1}{\tau p^{\beta}}\|\bar{X}_1-\bar{X}_2\|^2
    %\sim\sum_{i=1}^r\frac{\lambda_i+\sigma^2}{p^{\beta}}Z_i
    %+\frac{\sigma^2}{p^{\beta}}W.
    %$$
    %It's easy to see that 
    %$$\sum_{i=1}^r\frac{\lambda_i+\sigma^2}{p^{\beta}}Z_i\xrightarrow{\mathcal{L}}\sum_{i=1}^r l_i Z_i.$$
    %By central limit theorem, we have
    %$$
    %\frac{1}{\sqrt{2(p-r)}}\big( W-(p-r)\big)\xrightarrow{\mathcal{L}}N(0,1).
    %$$
%
    %If $\beta=1/2$, by Slutsky's theorem we have
    %$$
    %\frac{1}{\tau p^{\beta}}\|\bar{X}_1-\bar{X}_2\|^2-\frac{\sigma^2(p-r)}{p^\beta}\xrightarrow{\mathcal{L}}
    %\sum_{i=1}^r l_i Z_i+\sqrt{2}\sigma^2 \epsilon.
    %$$
    %where $\epsilon\sim N(0,1)$

\end{proof}

\section{Proofs of the results in Section~\ref{methodology}}\label{appendixB}

\begin{lemma}[Weyl's inequality]
Let $\BA$ and $\BB$ be two symmetric $n\times n$ matrices and $\BC=\BA+\BB$. If $r+s-1 \leq  i\leq j+k-n$, we have
\begin{equation*}
\lambda_j(\BA)+\lambda_k(\BB)\leq \lambda_i(\BC) \leq \lambda_r(\BA)+\lambda_s(\BB).
\end{equation*}
    See, for example,~\cite{Horn1985Matrix}, Theorem $4.3.1$.
\end{lemma}
%\begin{corollary}\label{WeylCor}
    %Let $H$ and $P$ be two symmetric matrices and $M=H+P$. If $\mathrm{rank}(P)< k\leq n$, then
    %\begin{equation*}
        %\lambda_k(M)\leq \lambda_1(H).
    %\end{equation*}
%\end{corollary}

\begin{lemma}[\cite{Cai2015Optimal}, Proposition 1]\label{pert}
    Let $\BA_1$ and $\BA_2$ be $p\times p$ symmetric matrices. Let $r<p$ be arbitrary and let $\BV_1, \BV_2\in \mathbb{O}_{p,r}$ be formed by the $r$ leading singular vectors of $\BA_1$ and $\BA_2$, respectively.
    Then
    $$
    \|\BA_1-\BA_2\|\geq \frac{1}{2}(\lambda_r(\BA_2)-\lambda_{r+1}(\BA_2))\|\BV_1 \BV_1^T- \BV_2 \BV_2^T\|.
    $$
\end{lemma}


\begin{lemma}[\cite{DAVIDSON2001317}, Theorem II.7]\label{DSbound}
    Let $\BZ$ be a $p\times n$ random matrix with i.i.d. $N(0,1)$ entries.
    Then for any $t>0$,
    \begin{align*}
        &\Pr(\sqrt{\lambda_1(\BZ \BZ^T )}>\sqrt{n}+\sqrt{p}+t)\leq e^{-t^2/2},
        \\
        &\Pr(\sqrt{\lambda_{\min(n,p)}(\BZ \BZ^T )}<\sqrt{n}-\sqrt{p}-t)\leq e^{-t^2/2}.
    \end{align*}
\end{lemma}
    We give two useful corollaries of Lemma~\ref{DSbound}.
\begin{corollary}\label{maxEigen}
    Suppose that $\BW_n$ is a $p \times p$ random matrix which is distributed as $\mathrm{Wishart}_p(n,\BI_{p})$. Then as $n,p\to \infty$,
    $$
        \lambda_1(\BW_n)=O_P(\max(n,p)).
    $$
\end{corollary}
\begin{proof}[\textbf{Proof}]
    The result follows from the inequality
    $$
    \begin{aligned}
        &\Pr\Big(\frac{\lambda_1(\BW_n)}{\max(n,p)}>16\Big)
        \leq
        \Pr\Big(\lambda_1(\BW_n)>8(n+p)\Big)
        \leq
        \Pr\Big(\lambda_1(\BW_n)>4(\sqrt{n}+\sqrt{p})^2\Big)\\
        =
        &\Pr\Big(\sqrt{\lambda_1(\BW_n)}>2(\sqrt{n}+\sqrt{p})\Big)
        \leq
        \Pr\Big(\sqrt{\lambda_1(\BW_n)}>2\sqrt{n}+\sqrt{p}\Big)\leq e^{-n/2},
    \end{aligned}
    $$
    where the last inequality follows from Lemma~\ref{DSbound} with $t=\sqrt{n}$.
\end{proof}

\begin{corollary}\label{corNorm}
Suppose that $\BW_n$ is a $p \times p$ random matrix which is distributed as $\mathrm{Wishart}_p(n,\BI_{p})$.
    Then as $n,p\to \infty$,
$$
    \|\frac{1}{n}\BW_n-\BI_p\|=O_P\Big(\max\big(\sqrt{\frac{p}{n}},\frac{p}{n}\big)\Big).
$$
\end{corollary}
\begin{proof}
    Since the eigenvalues of $n^{-1}\BW_n-\BI_p$ are $n^{-1}\lambda_1(\BW_n)-1,\ldots, n^{-1}\lambda_p(\BW_n)-1$, we have
     $$\|\frac{1}{n}\BW_n-\BI_p\|=\max\Big(\frac{1}{n}\lambda_1(\BW_n)-1,1-\frac{1}{n}\lambda_p(\BW_n)\Big).$$
This, together with the union bound, yields
$$
    \Pr\Big(\|\frac{1}{n}\BW_n-\BI_p\|>4\big(\sqrt{\frac{p}{n}}+\frac{p}{n}\big)\Big)
    \leq
    \Pr\Big(\lambda_1(\BW_n)>\big(\sqrt{n}+2\sqrt{p}\big)^2\Big)+
    \Pr\Big(\lambda_p(\BW_n)<n-4\sqrt{np}-4p\Big).
$$
    For the first term, we have    $$
    \Pr\Big(\lambda_1(\BW_n)>\big(\sqrt{n}+2\sqrt{p}\big)^2\Big)=
    \Pr\Big(\sqrt{\lambda_1(\BW_n)}>\sqrt{n}+2\sqrt{p}\Big)\leq e^{-p/2},
    $$
 where the last inequality follows from Lemma~\ref{DSbound} with $t=\sqrt{p}$.

    We now show that the second term is also bounded by $e^{-p/2}$.
    To see this, note that if $p>n/4$, then $n-4\sqrt{np}-4p\leq n-4p<0$.
    In this case, $\Pr\Big(\lambda_p(\BW_n)<n-4\sqrt{np}-4p\Big)=0$.
    If $p\leq n/4$, we have
    $$
    \begin{aligned}
        &\Pr\Big(\lambda_p(\BW_n)<n-4\sqrt{np}-4p\Big)
    \leq
    \Pr\Big(\lambda_p(\BW_n)<n-4\sqrt{np}+4p\Big)\\
        =&
    \Pr\Big(\sqrt{\lambda_p(\BW_n)}<\sqrt{n}-\sqrt{2p}\Big)
        \leq e^{-p/2},
    \end{aligned}
    $$
    where the last inequality follows from Lemma~\ref{DSbound} with $t=\sqrt{p}$.

    Now we conclude that
    $$
    \Pr\Big(\|\frac{1}{n}\BW_n-\BI_p\|>4\big(\sqrt{\frac{p}{n}}+\frac{p}{n}\big)\Big)
    \leq 2 e^{-p/2}.
    $$
    Consequently,
    $$\|\frac{1}{n}\BW_n-\BI_p\|=O_P\Big(\sqrt{\frac{p}{n}}+\frac{p}{n}\Big)=O_P\Big(\max\big(\sqrt{\frac{p}{n}},\frac{p}{n}\big)\Big).$$
\end{proof}



\begin{lemma}\label{eigenconsisLemma}
    Under Assumption~\ref{theModel}, we have
    \begin{equation}\label{eigenconsisResult1}
        \lambda_i(\BS)= \blambda_i+\frac{p+n-r-2}{n-2}\sigma^2+
        O_P\Big(\max\big(\frac{p^{\beta}}{\sqrt{n}},1\big)\Big),\quad i=1,\ldots, r,
    \end{equation}
    and
    \begin{equation}\label{eigenconsisResult2}
        \hat{\sigma}^2=
(1-\frac{r}{n-2})\sigma^2+
O_P\Big(\max\big(\frac{1}{\sqrt{np}},\frac{1}{p}\big)\Big).
    \end{equation}
\end{lemma}
\begin{proof}[\textbf{Proof}]
    Let $\bSigma=\BU\BE\BU^T$ denote the spectral decomposition of $\bSigma$, where
     $\BU=(\BV,\tilde{\BV})$ and $\BE=\mydiag(\blambda_1+\sigma^2,\ldots,\blambda_r+\sigma^2,\sigma^2,\ldots,\sigma^2)$.
%Denote by $\BS=\hat{\BU}\hat{\BE}\hat{\BU}^T$ the spectral decomposition of $\BS$, where $\hat{\BU}=(\hat{\BV},\hat{\tilde{\BV}})$ and $\hat{\BE}=\mydiag(\lambda_1(\BS),\ldots,\lambda_p(\BS))$.
Let $\bZ$ be a $p\times (n-2)$ random matrix with i.i.d. $N(0,1)$ entries.
Denote $\bZ={(\bZ_{(1)}^T,\bZ_{(2)}^T)}^T$, where $\bZ_{(1)}$ and $\bZ_{(2)}$ are the first $r$ rows and last $p-r$ rows of $\bZ$. 
Then the sample covariance matrix $\BS$ has the same distribution as the random matrix
$
    {(n-2)}^{-1} \BU \BE^{1/2} \bZ \bZ^T \BE^{1/2} \BU^T
$.
    So we have $\lambda_i(\BS)\sim (n-2)^{-1}\lambda_i(\bZ^T \BE \bZ)$, $i=1,\ldots,r$ and $\mytr(\BS)\sim (n-2)^{-1}\mytr(\BZ^T \BE \BZ)$.

    To prove~\eqref{eigenconsisResult1}, we only need to consider $(n-2)^{-1}\lambda_i(\bZ^T \BE \bZ)$.
Note that
$
    \bZ^T \BE \bZ= \bZ_{(1)}^T (\bLambda +\sigma^2 \BI_r) \bZ_{(1)}+
\sigma^2 \bZ_{(2)}^T  \bZ_{(2)}
$.
From this and Weyl's inequality, 
for $i=1,\ldots, r$, we have 
$$
    \begin{aligned}
        &\Big|\frac{1}{n-2}\lambda_i(\BZ^T \BE \BZ)
    -\frac{1}{n-2}\lambda_i\big(\BZ_{(1)}^T (\bLambda+\sigma^2 \BI_r)\BZ_{(1)}\big)
    -\frac{p-r}{n-2}\sigma^2
    \Big|
        \\
        =&\Big|\lambda_i\big(\frac{1}{n-2}\BZ^T \BE \BZ\big)
        -\lambda_i\big(\frac{1}{n-2}\BZ_{(1)}^T (\bLambda+\sigma^2 \BI_r)\BZ_{(1)}+\frac{p-r}{n-2}\sigma^2 \BI_{n-2}\big)
    \Big|
        \\
        \leq &
        \Big\|\frac{1}{n-2}\BZ^T \BE \BZ
    -\frac{1}{n-2}\BZ_{(1)}^T (\bLambda+\sigma^2 \BI_r)\BZ_{(1)}
        -\frac{p-r}{n-2}\sigma^2\BI_{n-2}
        \Big\|
        \\
        = &
        \Big\|
        \frac{1}{n-2}\sigma^2 \bZ_{(2)}^T  \bZ_{(2)}
        -\frac{p-r}{n-2}\sigma^2\BI_{n-2}
        \Big\|
        \\
        = &
        \frac{p-r}{n-2}
\sigma^2
        \Big\|
        \frac{1}{p-r}
         \bZ_{(2)}^T  \bZ_{(2)}
        -\BI_{n-2}
        \Big\|.
    \end{aligned}
$$
    But Corollary~\ref{corNorm} implies that
    $$
        \Big\|
        \frac{1}{p-r}
         \bZ_{(2)}^T  \bZ_{(2)}
        -\BI_{n-2}
        \Big\|=O_P\Big(\max\big(\sqrt{\frac{n-2}{p-r}},\frac{n-2}{p-r}\big)\Big).
    $$
    Thus, for $i=1,\ldots,r$,
\begin{equation}\label{toutengjiayou1}
\frac{1}{n-2}\lambda_i(\BZ^T \BE \BZ)
=\frac{1}{n-2}\lambda_i\big(\BZ_{(1)}^T (\bLambda+\sigma^2 \BI_r)\BZ_{(1)}\big)
+\frac{p-r}{n-2}\sigma^2
+
O_P\Big(\max\big(\sqrt{\frac{p}{n}},1\big)\Big).
\end{equation}
    Next we deal with $(n-2)^{-1}\lambda_i\big(\BZ_{(1)}^T (\bLambda+\sigma^2 \BI_r)\BZ_{(1)}\big)$.
    For $i=1,\ldots, r$, we have
$$
    \begin{aligned}
        &\Big|\frac{1}{n-2}\lambda_i \big(\bZ_{(1)}^T(\bLambda+\sigma^2 \BI_r)\bZ_{(1)}\big)-(\blambda_i +\sigma^2)\Big|\\
        =&
        \Big|\frac{1}{n-2}\lambda_i \big((\bLambda+\sigma^2 \BI_r)^{1/2}\bZ_{(1)}\bZ_{(1)}^T(\bLambda+\sigma^2 \BI_r)^{1/2}\big)
        -\lambda_i\big(\bLambda+\sigma^2 \BI_r\big)\Big|\\
        \leq &
        \Big\|
        \frac{1}{n-2}(\bLambda+\sigma^2 \BI_r)^{1/2}\bZ_{(1)}\bZ_{(1)}^T(\bLambda+\sigma^2 \BI_r)^{1/2}
        -(\bLambda+\sigma^2 \BI_r)
        \Big\|\\
        = &
        \Big\|
        (\bLambda+\sigma^2 \BI_r)^{1/2}(\frac{1}{n-2}\bZ_{(1)}\bZ_{(1)}^T-\BI_r)(\bLambda+\sigma^2 \BI_r)^{1/2}
        \Big\|\\
        \leq &
        (\blambda_1 + \sigma^2)
        \Big\|\frac{1}{n-2}\bZ_{(1)}\bZ_{(1)}^T-\BI_r\Big\|\\
        = &
        O_P\Big(\frac{\blambda_1}{\sqrt{n}}\Big),
    \end{aligned}
    $$
    where the first inequality follows from Weyl's inequality and the last equality follows from Corollary~\ref{corNorm}.
    This, together with~\eqref{toutengjiayou1}, leads to
$$
\frac{1}{n-2}\lambda_i(\BZ^T \BE \BZ)=
\blambda_i+\frac{p+n-r-2}{n-2}\sigma^2+O_P\Big(\max\big(\sqrt{\frac{p}{n}},\frac{\blambda_1}{\sqrt{n}},1\big)\Big),\quad i=1,\ldots, r.
$$
Then~\eqref{eigenconsisResult1} follows from $\blambda_1\asymp p^{\beta}$ and $\beta\geq 1/2$.
    
    Next we prove~\eqref{eigenconsisResult2}.
    We note that
    $$
    \hat{\sigma}^2=\frac{1}{p-r}\Big(\mytr(\BS)-\sum_{i=1}^r \lambda_i(\BS)\Big)
    \sim
\frac{1}{(n-2)(p-r)}\Big(\mytr(\BZ^T \BE \BZ)-\sum_{i=1}^r \lambda_i(\BZ^T \BE \BZ)\Big).
    $$
    For $\mytr(\BZ^T \BE \BZ)$, we have
    $
            \mytr\big(\bZ^T \BE \bZ\big)= \mytr\big(\bZ_{(1)}^T (\bLambda +\sigma^2 \BI_r) \bZ_{(1)}\big)+
       \sigma^2 \mytr\big( \bZ_{(2)}^T  \bZ_{(2)}\big)
    $.
    Since $\mytr\big( \bZ_{(2)}^T  \bZ_{(2)}\big)\sim \chi^2_{(n-2)(p-r)}$, by central limit theorem, we have
    $$\mytr\big( \bZ_{(2)}^T  \bZ_{(2)}\big)=(n-2)(p-r)+O_P(\sqrt{np}).$$
    It follows that
    \begin{equation}\label{toutengjiayou2}
        \begin{aligned}
            \mytr\big(\bZ^T \BE \bZ\big)&= \mytr\big(\bZ_{(1)}^T (\bLambda +\sigma^2 \BI_r) \bZ_{(1)}\big)+
            (n-2)(p-r)\sigma^2+O_P(\sqrt{np}).
        \end{aligned}
    \end{equation}
    In view of~\eqref{toutengjiayou1}, we have
    $$
\sum_{i=1}^r\lambda_i(\BZ^T \BE \BZ)
=\mytr\big(\BZ_{(1)}^T (\bLambda+\sigma^2 \BI_r)\BZ_{(1)}\big)
+r(p-r)\sigma^2
+
O_P\Big(\max\big(\sqrt{np},n\big)\Big).
    $$
    This, together with~\eqref{toutengjiayou2}, yields
    $$
\frac{1}{(n-2)(p-r)}\Big(\mytr(\BZ^T \BE \BZ)-\sum_{i=1}^r \lambda_i(\BZ^T \BE \BZ)\Big)=
(1-\frac{r}{n-2})\sigma^2+
O_P\Big(\max\big(\frac{1}{\sqrt{np}},\frac{1}{p}\big)\Big).
    $$
\end{proof}

\begin{lemma}\label{conRateLemma}
    Under Assumption~\ref{theModel}, we have
\begin{equation*}
\|\hat{\BV}\hat{\BV}^T-\BV\BV^T\|^2 =O_P(\frac{p}{p^{\beta}n}).
\end{equation*}
\end{lemma}
\begin{remark}
it has been proved in~\cite{Cai2012Sparse}, Theorem 5 that
 under certain conditions,
$$
\myE \|\hat{\BV}\hat{\BV}^T-\BV\BV^T\|_F^2 =O(\frac{p}{p^{\beta}n}),
    $$
    where $\|\cdot\|_F$ is the Frobenius norm.
    Moreover, they proved that the convergence rate $p/(p^{\beta}n)$ is in fact minimax optimal.
    %Note that the rank of $\hat{\BV}\hat{\BV}^T-\BV\BV^T$ is at most $2r$. 
    %Since we have assumed $r$ is fixed, the convergence in $\|\cdot\|_F$ is equivalent to the convergence in $\|\cdot\|$.
    %It seems that Lemma~\ref{conRateLemma} is a corollary of~\cite{Cai2012Sparse}, Theorem 5.
    However,~\cite{Cai2012Sparse}, Theorem 5 needs the condition $\log p=O(n)$ which is unwanted.
    This condition is used to control the expectation and hence can be dropped in Lemma~\ref{conRateLemma}.
\end{remark}
\begin{proof}[\textbf{Proof}]
    Since $\|\hat{\BV}\hat{\BV}^T-\BV\BV^T\|^2\leq 1$, the conclusion is trivial when $p/(p^{\beta} n)$ is unbounded.
    So without loss of generality, we assume $p/(p^{\beta} n)=O(1)$.
    Define $\BU$, $\BE$, $\bZ$, $\bZ_{(1)}$ and $\bZ_{(2)}$ as in the proof of Lemma~\ref{eigenconsisLemma}.
Without loss of generality, we assume that $\BS= (n-2)^{-1}\BU\BE^{1/2}\bZ \bZ^T \BE^{1/2} \BU^T$. 
    Similar to the proof of~\cite{Cai2012Sparse}, Theorem 5,
we define 
    $$
    \BS_0=
        \frac{1}{n-2}\BV(\bLambda+\sigma^2\BI_r)^{1/2}\bZ_{(1)} \bZ_{(1)}^T(\bLambda+\sigma^2\BI_r)^{1/2}\BV^T
        +\sigma^2\tilde{\BV}\tilde{\BV}^T.
    $$
    It can be seen that the set of eigenvalues of $\BS_0$ is the union of the nonzero eigenvalues of the matrix $(n-2)^{-1}\BV(\bLambda+\sigma^2\BI_r)^{1/2}\bZ_{(1)} \bZ_{(1)}^T(\bLambda+\sigma^2\BI_r)^{1/2}\BV^T$ and $\sigma^2\tilde{\BV}\tilde{\BV}^T$.
All the nonzero eigenvalues of $\sigma^2\tilde{\BV}\tilde{\BV}^T$ are $\sigma^2$.
We note that with probability $1$, the first matrix is of rank $r$.
        Define the event
        $$
        A=
        \Big\{
        \frac{1}{n-2}\lambda_r\big(\BV(\bLambda+\sigma^2 \BI_r)^{1/2}\bZ_{(1)} \bZ_{(1)}^T(\bLambda+\sigma^2 \BI_r)^{1/2}\BV^T\big)
        >\sigma^2
        \Big\}.
        $$
        On the event $A$, the eigenspace of $\BS_0$ associated with the $r$ leading eigenvalues is exactly $\BV \BV^T$, although the individual columns of $\BV$ need not be the leading eigenvectors of $\BS_0$.
    Applying Lemma~\ref{pert} to $\BS$ and $\BS_0$ yields
    \begin{equation}\label{toutengjiayou3}
    \|\hat{\BV}\hat{\BV}^T - \BV\BV ^T\| \mathbf{1}_{\{A\}}\leq
        \frac{2}{\lambda_r(\BS_0)-\lambda_{r+1}(\BS_0)}\|\BS-\BS_0\|\mathbf{1}_{\{A\}}.
    \end{equation}
    Note that
$$
        \lambda_r\Big(\frac{1}{n-2}\BV(\bLambda+\sigma^2\BI_r)^{1/2}\bZ_{(1)} \bZ_{(1)}^T(\bLambda+\sigma^2\BI_r)^{1/2}\BV^T\Big)
    =
        \frac{1}{n-2}\lambda_r\big((\bLambda+\sigma^2\BI_r)^{1/2}\bZ_{(1)} \bZ_{(1)}^T(\bLambda+\sigma^2\BI_r)^{1/2}\big).
$$
By Weyl's inequality,
$$
    \begin{aligned}
        &\Big|
        \frac{1}{n-2}\lambda_r((\bLambda+\sigma^2 \BI_r)^{1/2}\bZ_{(1)} \bZ_{(1)}^T(\bLambda+\sigma^2 \BI_r)^{1/2})
    -
        (\blambda_r+\sigma^2)
    \Big|\\
        =&\Big|
    \frac{1}{n-2}\lambda_r((\bLambda+\sigma^2 \BI_r)^{1/2}\bZ_{(1)} \bZ_{(1)}^T(\bLambda+\sigma^2 \BI_r)^{1/2})
    -
    \lambda_r(\bLambda+\sigma^2\BI_r)
    \Big|
        \\
        \leq&
        \Big\|(\bLambda+\sigma^2\BI_r)^{1/2}\big(\frac{1}{n-2}\bZ_{(1)} \bZ_{(1)}^T-\BI\big)(\bLambda+\sigma^2\BI_r)^{1/2}\Big\|\\
        \leq& (\blambda_1+\sigma^2)\Big\|\frac{1}{n-2}\bZ_{(1)} \bZ_{(1)}^T-\BI\Big\|
        \\
        =& O_P\Big(\frac{\blambda_1+\sigma^2}{\sqrt{n}}\Big),
    \end{aligned}
$$
    where the last equality follows from Corollary~\ref{corNorm}.
    Hence
$$
        \frac{1}{n-2}\lambda_r(\BV(\bLambda+\sigma^2 \BI_r)^{1/2}\bZ_{(1)} \bZ_{(1)}^T(\bLambda+\sigma^2 \BI_r)^{1/2}\BV^T)
        =(\blambda_1+\sigma^2)\Big(1+O_P\big(\frac{1}{\sqrt{n}}\big)\Big).
        $$
        It follows that $P(A)\to 1$.
        In view of~\eqref{toutengjiayou3}, it is sufficient to show that
        $$
        \frac{2}{\lambda_r(\BS_0)-\lambda_{r+1}(\BS_0)}\|\BS-\BS_0\|\mathbf{1}_{\{A\}}=O_P(\sqrt{\frac{p}{p^{\beta}n }}).
        $$
        Note that on event $A$, $\lambda_{r+1}(\BS_0)=\sigma^2$ and
        $$\lambda_r(\BS_0)=\lambda_r((n-2)^{-1}\lambda_r(\BV(\bLambda+\sigma^2 \BI_r)^{1/2}\bZ_{(1)} \bZ_{(1)}^T(\bLambda+\sigma^2 \BI_r)^{1/2}\BV^T)).$$
        Hence
$$
\begin{aligned}
    &\frac{2}{\lambda_r(\BS_0)-\lambda_{r+1}(\BS_0)}\mathbf{1}_{\{A\}}
    =
    \frac{2}{
        \lambda_r(\frac{1}{n-2}\lambda_r(\BV(\bLambda+\sigma^2 \BI_r)^{1/2}\bZ_{(1)} \bZ_{(1)}^T(\bLambda+\sigma^2 \BI_r)^{1/2}\BV^T))-\sigma^2}\mathbf{1}_{\{A\}}\\
    =&\frac{2}{\blambda_r\Big(1+O_P\big(\frac{1}{\sqrt{n}}\big)\Big)}\mathbf{1}_{\{A\}}
    =\frac{2}{\blambda_r}(1+o_P(1)).
\end{aligned}
    $$
    Next we bound $\|\BS-\BS_0\|$. 
    We have
    $$
    \begin{aligned}
        &\|\BS-\BS_0\|=
        \|(\BV\BV^T+\tilde{\BV}\tilde{\BV}^T)(\BS-\BS_0)(\BV\BV^T+\tilde{\BV}\tilde{\BV}^T)\|\\
        \leq& \|\BV\BV^T (\BS-\BS_0) \BV\BV^T\|+2 \|\BV\BV^T (\BS-\BS_0) \tilde{\BV}\tilde{\BV}^T\|+\|\tilde{\BV}\tilde{\BV}^T (\BS-\BS_0) \tilde{\BV}\tilde{\BV}^T\|\\
        \leq& \|\BV^T (\BS-\BS_0) \BV\|+2 \|\BV^T (\BS-\BS_0) \tilde{\BV}\|+\|\tilde{\BV}^T (\BS-\BS_0) \tilde{\BV}\|\\
        = &
        2\big\|\frac{\sigma}{n-2}(\bLambda+\sigma^2 \BI_r)^{1/2}\bZ_{(1)} \bZ_{(2)}^T\big\|+
        \sigma^2\big\|\frac{1}{n-2}\bZ_{(2)} \bZ_{(2)}^T- \BI_{p-r}\big\|\\
        \leq & \frac{2\sqrt{(\lambda_1+\sigma^2)\sigma^2}}{n-2}\|\bZ_{(1)}\bZ_{(2)}^T\|+
        \sigma^2\big\|\frac{1}{n-2}\bZ_{(2)} \bZ_{(2)}^T- \BI_{p-r}\big\|.
    \end{aligned}
    $$
    By Corollary~\ref{corNorm}, we have $\big\|(n-2)^{-1}\bZ_{(2)} \bZ_{(2)}^T- \BI_{p-r}\big\|=O_p(\max(\sqrt{p/n},p/n))$.
    By the independence of $\bZ_{(1)}$ and $\bZ_{(2)}$, we have
    $$
    \begin{aligned}
        &\myE \|\bZ_{(1)}\bZ_{(2)}^T\|^2\leq
    \myE \|\bZ_{(1)}\bZ_{(2)}^T\|_F^2
    =
    \myE \big[ \mytr\big(\bZ_{(1)}\bZ_{(2)}^T\bZ_{(2)}\bZ_{(1)}^T\big) \big]\\
        =&
        \myE\myE \big[ \mytr\big(\bZ_{(1)}\bZ_{(2)}^T\bZ_{(2)}\bZ_{(1)}^T\big)\big|\BZ_{(1)} \big]
        =(p-r)
    \myE \big[ \mytr\big(\bZ_{(1)}\bZ_{(1)}^T\big) \big]
        =r(n-2)(p-r).
    \end{aligned}
    $$
    Hence $\|\bZ_{(1)}\bZ_{(2)}^T\|=O_P(\sqrt{np})$.
    Combining these bounds leads to
    $$
    \|\BS-\BS_0\|=
 O_P(\sqrt{\frac{\lambda_1 p}{n}})+O_P\Big(\max\big(\sqrt{\frac{p}{n}},\frac{p}{n}\big)\Big)
    =
    O_P(\sqrt{\frac{ p^{\beta}p}{n}})+O_P(\frac{p}{n}).
    $$
    Thus,
    $$
   \frac{2}{\lambda_r(\BS_0)-\lambda_{r+1}(\BS_0)}\|\BS-\BS_0\|=
    O_P(\sqrt{\frac{p}{p^{\beta}n}})+O_P(\frac{p}{p^{\beta}n})=
O_P(\sqrt{\frac{p}{p^{\beta}n}}),
    $$
    where the last equality holds since we have assumed $p/(p^{\beta}n)=O(1)$.
    This completes the proof.

\end{proof}


\begin{proof}[\textbf{Proof of Proposition~\ref{oracleTheorem}}]
Note that
    \begin{equation}\label{qiguaile}
        \begin{aligned}
            &\|\tilde{\BV}^T(\bar{X}_1-\bar{X}_2)\|^2
            =\big\|\tilde{\BV}^T(\mu_1-\mu_2)+\tilde{\BV}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)\big\|^2\\
            =&\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2+
            \big\|\tilde{\BV}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)\big\|^2+
            2{(\mu_1-\mu_2)}^T\tilde{\BV}\tilde{\BV}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)\\
            =&\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2+
            \big\|\tilde{\BV}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)\big\|^2+
            o_P(\frac{\sqrt{p}}{n}).
        \end{aligned}
    \end{equation}
    The last equality holds since
    \begin{equation*}
        \begin{aligned}
            &2{(\mu_1-\mu_2)}^T\tilde{\BV}\tilde{\BV}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)\sim N(0,4\sigma^2 \tau \|\tilde{\BV}^T(\mu_1-\mu_2)\|^2)\\
            =& O_P(\sqrt{\tau}\|\tilde{\BV}^T(\mu_1-\mu_2)\| )=o_P(\frac{\sqrt{p}}{n}).
        \end{aligned}
    \end{equation*}

    %Let $Y_{k,i}=\tilde{\BV}^T (X_{k,i}-\mu_k)$, $i=1,\ldots,n_k$, $k=1,2$.
    %Then $Y_{k,i}\sim N(\tilde{\BV}^T\mu_k,\sigma^2 \BI_{p-r})$.
    %Let $\bar{Y}_1$ and $\bar{Y}_2$ be the sample means of $\{Y_{1,i}\}_{i=1}^{n_1}$ and $\{Y_{2,i}\}_{i=1}^{n_2}$ respectively. 
    %Then
    %\begin{equation}\label{prop1eq1}
        %\begin{aligned}
            %&\|\tilde{\BV}^T(\bar{X}_1-\bar{X}_2)\|^2
            %=\|\tilde{\BV}^T(\mu_1-\mu_2)+(\bar{Y}_1-\bar{Y}_2)\|^2\\
            %=&\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2+\|\bar{Y}_1-\bar{Y}_2\|^2+
            %2{(\mu_1-\mu_2)}^T\tilde{\BV}(\bar{Y}_1-\bar{Y}_2)\\
            %=&\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2+\|\bar{Y}_1-\bar{Y}_2\|^2+
            %o_P(\frac{\sqrt{p}}{n}).
        %\end{aligned}
    %\end{equation}
    %The last equality holds since
    %\begin{equation*}
        %\begin{aligned}
            %&2{(\mu_1-\mu_2)}^T\tilde{\BV}(\bar{Y}_1-\bar{Y}_2)\sim N(0,4\sigma^2 \tau \|\tilde{\BV}^T(\mu_1-\mu_2)\|^2)\\
            %=& O_P(\sqrt{\tau}\|\tilde{\BV}^T(\mu_1-\mu_2)\| )=o_P(\frac{\sqrt{p}}{n}).
        %\end{aligned}
    %\end{equation*}
    For $k=1,2$, we have
    \begin{equation*}
        \begin{aligned}
            &\frac{1}{n_k} \mytr(\tilde{\BV}^T \BS_k \tilde{\BV})\sim \frac{\sigma^2}{n_k(n_k-1)}\chi^2_{(p-r)(n_k-1)}
            =
            \sigma^2\frac{p-r}{n_k}\Big(1+O_P\big(\frac{1}{\sqrt{(p-r)(n_k-1)}}\big)\Big),
        \end{aligned}
    \end{equation*}
    where the last equality comes from central limit theorem. 
    Thus,
    \begin{equation}\label{prop1eq2}
        \begin{aligned}
            &\frac{1}{n_1} \mytr(\tilde{\BV}^T \BS_1 \tilde{\BV})+
            \frac{1}{n_2} \mytr(\tilde{\BV}^T \BS_2 \tilde{\BV})=\sigma^2 \tau (p-r)+o_P(\frac{\sqrt{p}}{n}).
        \end{aligned}
    \end{equation}
It follows from~\eqref{qiguaile} and~\eqref{prop1eq2} that
            $$
            \frac{T_1-\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2}{\sigma^2\sqrt{2\tau^2 p}}
            =
            \frac{\|\tilde{\BV}^T \big( (\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2) \big)\|^2-
                \sigma^2 \tau (p-r)}{\sigma^2\sqrt{2\tau^2 p}}
                +o_P(1).
    $$
Then the proposition follows from
$\|\tilde{\BV}^T \big( (\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2) \big)\|^2\sim \sigma^2\tau\chi^2_{p-r}$.
\end{proof}



\begin{proof}[\textbf{Proof of Proposition~\ref{varianceEstimation}}]
    The conclusion~\eqref{varianceEstimationPropResult1} is a direct corollary of~\eqref{eigenconsisResult1} in Lemma~\ref{eigenconsisLemma}.
    By~\eqref{eigenconsisResult2} and~\eqref{varianceEstimationPropResult1}, for $i=1,\ldots, r$, we have 
    $$
    \begin{aligned}
        \hat{\blambda}_i=&
        \lambda_i(\BS)-\frac{p+n-r-2}{n-2}\hat{\sigma}_*^2\\
        =& \blambda_i +\frac{p+n-r-2}{n-2}\sigma^2+O_P\Big(\max\big(\frac{p^{\beta}}{\sqrt{n}},1\big)\Big)
        -\frac{p+n-r-2}{n-2}\sigma^2-O_P\Big( \frac{n+p}{n}\max\big(\frac{1}{\sqrt{np}},\frac{1}{p}\big)\Big)\\
        =& \blambda_i+O_P\Big(\max\big(\frac{p^{\beta}}{\sqrt{n}},1\big)\Big),
    \end{aligned}
    $$
    which proves~\eqref{varianceEstimationPropResult2}.
\end{proof}



% proof of space estimation theorem
\begin{proof}[\textbf{Proof of Theorem~\ref{myPanpan}}]
    Proposition~\ref{varianceEstimation} implies that $\hat{\sigma}_{*}^2\xrightarrow{P} \sigma^2$ and $\hat{\blambda_i}/\blambda_i\xrightarrow{P} 1$.
    Hence
    $$
    \begin{aligned} 
 \tau\sum_{i=1}^r \frac{p\hat{\sigma}_*^2}{n\hat{\blambda}_i+(n+p)\hat{\sigma}_*^2}\hat{\blambda}_i
        =
        \tau\frac{p}{n}\sum_{i=1}^r \frac{\hat{\sigma}_*^2}{1+\frac{(n+p)}{n\hat{\blambda}_i}\hat{\sigma}_*^2}
        =
        \tau\frac{p}{n}\sum_{i=1}^r \frac{\sigma^2(1+o_P(1))}{1+\frac{(n+p)}{n\blambda_i}\sigma^2(1+o_P(1))}.
    \end{aligned}
    $$
    Since $p/n^2\to 0$ implies $p/(n\blambda_i)\to 0$, we have
    $$
    \begin{aligned} 
 \tau\sum_{i=1}^r \frac{p\hat{\sigma}_*^2}{n\hat{\blambda}_i+(n+p)\hat{\sigma}_*^2}\hat{\blambda}_i
        =r\tau \frac{p}{n} \sigma^2(1+o_P(1))=o_P(\sqrt{\tau^2 p}).
    \end{aligned}
    $$
    In view of~\eqref{varianceEstimationPropResult1}, we have
    $$
    \begin{aligned}
        &\tau(p-r)\hat{\sigma}_*^2 =
    \tau(p-r)\sigma^2 + O_P\Big(\frac{p}{n}\max\big(\frac{1}{\sqrt{np}},\frac{1}{p}\big)\Big)\\
        =&
        \tau(p-r)\sigma^2 + O_P\Big(\frac{\sqrt{p}}{n}\max\big(\frac{1}{\sqrt{n}},\frac{1}{\sqrt{p}}\big)\Big)
        =\tau(p-r)\sigma^2 + o_P(\sqrt{\tau^2 p}).
    \end{aligned}
    $$
Thus,
\begin{equation*}
    \begin{aligned}
        &\frac{T_2-\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2}{\sigma^2\sqrt{2\tau^2 p}}
        =
        \frac{\|\hat{\tilde{\BV}}^T(\bar{X}_1-\bar{X}_2)\|^2-\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2
        -\tau (p-r)\sigma^2
        }{\sigma^2\sqrt{2\tau^2 p}}+o_P(1),
    \end{aligned}
\end{equation*}
    We write
\begin{equation*}
    \begin{aligned}
        &\frac{\|\hat{\tilde{\BV}}^T(\bar{X}_1-\bar{X}_2)\|^2-\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2
        -\tau (p-r)\sigma^2
        }{\sigma^2\sqrt{2\tau^2 p}}
        \\
        =&\frac{1}{\sigma^2\sqrt{2\tau^2 p}}(
        P_1+P_2+P_3
        ),
    \end{aligned}
\end{equation*}
where
\begin{align*}
    P_1&=\|\hat{\tilde{\BV}}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)\|^2- \tau (p-r)\sigma^2,\\
    P_2&=2{(\mu_1-\mu_2)}^T \hat{\tilde{\BV}}\hat{\tilde{\BV}}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big),\\
    P_3&=\|\hat{\tilde{\BV}}^T(\mu_1-\mu_2)\|^2-\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2.
\end{align*}
To prove the theorem, it suffices to show that
$$
    \frac{P_1}{\sigma^2\sqrt{2\tau^2 p}}\xrightarrow{\mathcal{L}} N(0,1),
    \quad
    \frac{P_2}{\sigma^2\sqrt{2\tau^2 p}}\xrightarrow{P} 0
    \quad
    \textrm{and}
    \quad
    \frac{P_3}{\sigma^2\sqrt{2\tau^2 p}}\xrightarrow{P}0.
    $$
   First we deal with $P_2$.
   Let $\epsilon$ be any fixed positive number. 
   We have
   $$
   \Pr\Big(\frac{P_2}{\sigma^2\sqrt{2\tau^2 p}}>\epsilon\Big)
   =
   \myE\big[\Pr\big({P_2}>\epsilon{\sigma^2\sqrt{2\tau^2 p}}\big|\BS\big)\big].
   $$
   Since the conditional probability 
   $
   \Pr\big({P_2}>\epsilon{\sigma^2\sqrt{2\tau^2 p}}\big|\BS\big)
   $
   is bounded, by dominated convergence theorem, we only need to prove
   $\Pr\big({P_2}>\epsilon{\sigma^2\sqrt{2\tau^2 p}}\big|\BS\big)\xrightarrow{P}0$.
    Note that $\bar{X}_1$, $\bar{X}_2$, and $\BS$ are mutually independent and $\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T$ only depends on $\BS$.
    We have
    \begin{equation*}
        \begin{aligned}
            &\Pr\big({P_2}>\epsilon{\sigma^2\sqrt{2\tau^2 p}}\big|\BS\big)\leq
            \frac{1}{2\epsilon^2\sigma^4\tau^2 p}\myE \big(P_2^2\big|\BS\big)\\
            =&\frac{1}{2\epsilon^2\sigma^4\tau^2 p} 4\tau {(\mu_1-\mu_2)}^T \hat{\tilde{\BV}}\hat{\tilde{\BV}}^T\bSigma \hat{\tilde{\BV}}\hat{\tilde{\BV}}^T(\mu_1-\mu_2)\\
            \leq &
            \frac{2}{\epsilon^2\sigma^4\tau p}
             \lambda_1(\hat{\tilde{\BV}}^T\bSigma \hat{\tilde{\BV}}) {(\mu_1-\mu_2)}^T \hat{\tilde{\BV}}\hat{\tilde{\BV}}^T(\mu_1-\mu_2)\\
            \leq & 
\frac{2}{\epsilon^2\sigma^4\tau p}
             \|\mu_1-\mu_2\|^2
             \lambda_1(\hat{\tilde{\BV}}^T\bSigma \hat{\tilde{\BV}})\\
             =&
             O(\frac{1}{\sqrt{p}})
             \lambda_1(\hat{\tilde{\BV}}^T (\BV\bLambda \BV^T +\sigma^2 \BI_p) \hat{\tilde{\BV}})\\
             \leq &
             O(\frac{1}{\sqrt{p}})
             \big(\kappa p^{\beta}\lambda_1(\hat{\tilde{\BV}}^T \BV\BV^T  \hat{\tilde{\BV}})+\sigma^2\big).\\
        \end{aligned}
    \end{equation*}
    But
    \begin{equation*}
        \begin{aligned}
\lambda_1(\hat{\tilde{\BV}}^T \BV\BV^T  \hat{\tilde{\BV}})
=\|\BV^T  \hat{\tilde{\BV}}\|^2
            = \|\BV\BV^T-\hat{\BV}\hat{\BV}^T\|^2=O_P\Big(\frac{p}{p^{\beta}n}\Big),
        \end{aligned}
    \end{equation*}
    where the second equality follows from~\cite{matrixComputations}, Theorem 2.5.1 and the last equality follows from Lemma~\ref{conRateLemma}. 
    Thus,
    \begin{equation*}
        \begin{aligned}
            &\Pr\big({P_2}>\epsilon{\sigma^2\sqrt{2\tau^2 p}}\big|\BS\big)
             =
             O(\frac{1}{\sqrt{p}})
             \big(O_P(\frac{p}{n})+\sigma^2\big)
             =O(1)(O_P(\frac{\sqrt{p}}{n})+\frac{\sigma^2}{\sqrt{p}})
             =o_P(1).
        \end{aligned}
    \end{equation*}
    Next we deal with $P_3$.
    Note that
    \begin{equation*}
        \begin{aligned}
            &|P_3|=
            \big|{(\mu_1-\mu_2)}^T(\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T-\tilde{\BV}\tilde{\BV}^T)(\mu_1-\mu_2)\big|
            \leq 
            \|\mu_1-\mu_2\|^2 \|\hat{\tilde{\BV}}\hat{\tilde{\BV}}^T-\tilde{\BV}\tilde{\BV}^T\|\\
            =& 
            \|\mu_1-\mu_2\|^2  \|\hat{\BV}\hat{\BV}^T-\BV\BV^T\|
        =O(\frac{\sqrt{p}}{n})\sqrt{O_P(\frac{p}{p^{\beta}n})}=o_P(\frac{\sqrt{p}}{n}).
        \end{aligned}
    \end{equation*}
    Hence
    \begin{equation*}
        \begin{aligned}
            &\frac{P_3}{\sigma^2\sqrt{2\tau^2 p}}= O(\frac{n}{\sqrt{p}})P_3=o_P(1).
        \end{aligned}
    \end{equation*}

    Now we prove the asymptotic normality of $P_1$.
    To make clear the mode of convergence, we need a metric for weak convergence. For two distribution function $F$ and $G$, the Levy metric $\rho$ of $F$ and $G$ is defined as
    $$
   \rho(F,G) =\inf\{\epsilon:F(x-\epsilon)-\epsilon\leq G(x)\leq F(x+\epsilon)+\epsilon\quad \textrm{for all $x$}\}.
    $$
    It's well known that $\rho(F_n,F)\to 0$ if and only if $F_n\xrightarrow{\mathcal{L}}F$.

    Since the conditional distribution of
    $\hat{\tilde{\BV}}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)$ given $\BS$ is $N(0,\tau \hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})$,
    we have that
\begin{equation}\label{houjia2}
\tau^{-1}\big\|\hat{\tilde{\BV}}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)\big\|^2
\sim
    \sum_{i=1}^{p-r} \lambda_i(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})\xi_i^2,
\end{equation}
where $\xi_1,\ldots,\xi_{p-r}$ are i.i.d.\  standard normal random variables which are independent of $\hat{\tilde{\BV}}$.
    In view of Lemma~\ref{quadraticFormCLT}, the asymptotic distribution of $P_1$ relies on the asymptotic behavior of $\lambda_i(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})$, $i=1,\ldots, p-r$.
    As we have shown,
     \begin{equation}\label{houjia1}
     \lambda_1(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})\leq 
     \kappa p^{\beta}\lambda_1(\hat{\tilde{\BV}}^T \BV \BV^T\hat{\tilde{\BV}})+\sigma^2
     =
    \kappa p^\beta \|\BV\BV^T -\hat{\BV}\hat{\BV}^T\|^2+\sigma^2.
     \end{equation}
    On the other hand,
    for $i=r+1,\ldots,p-r$,
    we have
    \begin{equation}\label{houjia3}
    \lambda_i(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})
    =
    \lambda_i(\hat{\tilde{\BV}}^T \BV \bLambda \BV^T \hat{\tilde{\BV}})+\sigma^2
    =\sigma^2,
    \end{equation}
where the last equality follows from $\myrank(\hat{\tilde{\BV}}^T \BV \bLambda \BV^T \hat{\tilde{\BV}})\leq \myrank(\BV)=r$.
It follows from~\eqref{houjia1} and~\eqref{houjia3} that
\begin{equation}\label{traceA1}
    \begin{aligned}
        \mytr(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})^2=&
    O_P\Big(\big(p^{\beta}\|\BV\BV^T-\hat{\BV}\hat{\BV}^T\|^2+1\big)^2\Big)
    +
    (p-2r)\sigma^4\\
    =&
    {\big(\frac{p}{n}+1\big)}^2O_P(1)
    +
    (p-2r)\sigma^4
    =p\sigma^4(1+o_P(1)).
    \end{aligned}
\end{equation}
This, combined with~\eqref{houjia1}, yields
$$
\frac{\lambda_1^2(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})}{\mytr(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})^2}
=O_P\Big(p^{-1}\big(p^{\beta}\|\BV\BV^T-\hat{\BV}\hat{\BV}^T\|^2+1\big)^2\Big)
        =O_P\Big(\frac{{(p/n+1)}^2}{p}\Big)=o_P(1).
$$
Then for every subsequence $\{n(k)\}$ of $\{n\}$, there's a further subsequence $\big\{n\big(k(l)\big)\big\}$ along which
$$
\frac{\lambda_1^2(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})}{\mytr(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})^2}\xrightarrow{a.s.}0.
$$
This fact, together with~\eqref{houjia2} and Lemma~\ref{quadraticFormCLT}, implies that along $\big\{n\big(k(l)\big)\big\}$ we have
\begin{equation}\label{aseq}
    \rho\big(\mathcal{L}( Y_n |\,\BS),N(0,1)\big)\xrightarrow{a.s.}0,
\end{equation}
where 
$$
Y_n=\frac{\|\hat{\tilde{\BV}}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)\|^2-\tau\mathrm{tr}(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})}{\sqrt{2\tau^2\mathrm{tr}(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})^2}},
$$
and $\mathcal{L}(Y_n|\,\BS)$ is the conditional distribution of $Y_n$ given $\BS$.
By the definition of weak convergence,~\eqref{aseq} implies that for every continuous bounded function $f(\cdot)$, $\myE[f(Y_n)|\,\BS]\xrightarrow{a.s.}\myE[f(\xi^*)]$ along $\big\{n\big(k(l)\big)\big\}$, where $\xi^*$ is a standard normal random variable.
By dominated convergence theorem, $\myE[f(Y_n)]\to \myE[f(\xi^*)]$ along $\big\{n\big(k(l)\big)\big\}$.
This implies that $Y_n\xrightarrow{\mathcal{L}}N(0,1)$ along $\big\{n\big(k(l)\big)\big\}$.
Thus, for every subsequence of $\{n\}$, there is a further subsequence along which
$Y_n\xrightarrow{\mathcal{L}}N(0,1)$.
This means $Y_n\xrightarrow{\mathcal{L}}N(0,1)$, or
%$$
%\rho\Big(\mathcal{L}\Big(\frac{\|\hat{\tilde{\BV}}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)\|^2-\tau\mathrm{tr}(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})}{\sqrt{2\tau^2\mathrm{tr}(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})^2}}\Big|S\Big),N(0,1)\Big)\xrightarrow{P} 0.
%$$
$$
\frac{\|\hat{\tilde{\BV}}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)\|^2-\tau\mathrm{tr}(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})}{\sqrt{2\tau^2\mathrm{tr}(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})^2}}\xrightarrow{\mathcal{L}}N(0,1).
$$


By~\eqref{houjia1} and~\eqref{houjia3}, we have
\begin{equation}\label{traceA2}
    \begin{aligned}
        &\mytr(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})=
    \sum_{i=1}^r \lambda_i(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})
    +
    \sum_{i=r+1}^{p-r} \lambda_i(\hat{\tilde{\BV}}^T\bSigma\hat{\tilde{\BV}})\\
        =&
    O_P(\frac{p}{n}+1)+(p-2r)\sigma^2
        =
        (p-r)\sigma^2+o_P(\sqrt{p}).
    \end{aligned}
\end{equation}
It follows from~\eqref{traceA1},~\eqref{traceA2} and Slutsky's theorem that
$$
\frac{\|\hat{\tilde{\BV}}^T\big((\bar{X}_1-\mu_1)-(\bar{X}_2-\mu_2)\big)\|^2-\sigma^2\tau(p-r) }{\sigma^2\sqrt{2\tau^2 p}}\xrightarrow{\mathcal{L}}N(0,1).
$$
This completes the proof.
\end{proof}

\end{appendices}



%\begin{remark}
    %Compared with random projection method, our projection is determined by the structure of $S_1$, $S_2$ and $S$.
    %We don't  project multiple times as random projection method did, which leads to reproducibility in practice.
%\end{remark}


%\begin{remark} When both samples are simultaneously transformed by shift and orthogonal transformation, the statistic $T_2$ is invariant.
    %More precisely, $T_2$ is invariant under the following transformation: 
    %\begin{equation*}
        %\textrm{$X_{1,i}\mapsto OX_{1,i}+\mu$ and $X_{2,j}\mapsto OX_{2,j}+\mu$, $i=1,\ldots,n_1$, $j=1,\ldots,n_2$,}
    %\end{equation*}
    %where $\mu\in\mathbb{R}^p$ and $O\in\mathbb{O}_{p\times p}$.
%\end{remark}



    %Since the new test statistic estimates $\|\tilde{\BV}^T(\mu_1-\mu_2)\|^2$,
 %the superiority of the new test will be established if 
    
%\begin{equation}\label{yuedengyu}
    %\frac{\|\tilde{\BV}^T(\mu_1-\mu_2)\|}{\|\mu_1-\mu_2\|}\approx 1.
%\end{equation}
%Obviously,~\eqref{yuedengyu}
%is not always the case since there always exists some
%$\tilde{\BV}$ and $\mu_1-\mu_2$ such that $\|\tilde{\BV}^T(\mu_1-\mu_2)\|=0$.
%However,~\eqref{yuedengyu} is reasonable since $\tilde{\BV}\tilde{\BV}^T$ is nearly an identity matrix in the sense that
    %${\|\BI_p-\tilde{\BV}\tilde{\BV}^T\|_F^2}/{\|\BI_p\|_F^2}=r/p\to 0$. 
%In bayesian framework, if we assume that the elements of $\mu_k$ are independently generated from certain prior distribution, it can be established that 
    %${\|\tilde{\BV}(\mu_1-\mu_2)\|}/{\|\mu_1-\mu_2\|}\xrightarrow{P}1$.



%When $\frac{\sqrt{p}}{n_1+n_2}\to 0$, the critical value of our test can be approximated by it's asymptotic distribution which we will encounter later.
%However, it is a more practical issue to deal with the case when $n$ is small or the case when $p$ is much larger than $n$. In these cases, the null distribution is complicated and asymptotic distribution is a poor approximation of true distribution. Fortunately, permutation method can be used with the price of heavier computational burden. See~\cite{Lehmann}.


%The proof of Theorem~\ref{myPanpan} implies that when Assumption~\ref{pAndN} is not satisfied, the asymptotic normality is invalid. In this case, permutation method can be used to determine the critical value.
%We will see from simulation results that the new test has good power behavior even if $p$ is much large than $n$.

\section*{Acknowledgements}
This work was supported by the National Natural Science Foundation of China under Grant No. 11471035, 11471030.


\section*{References}

\bibliography{mybibfile}

\end{document}
